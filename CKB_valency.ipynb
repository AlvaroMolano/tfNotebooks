{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `שכב` valency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook last modified on 2017-01-02 19:08:44.310778\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "last_modified = datetime.now()\n",
    "print('Notebook last modified on {}'.format(last_modified.__str__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Methodology\n",
    "The goal is to inventory and categorise the various satellites surrounding the verb שכב (\"to lie\") in biblical Hebrew in order to determine which elements give rise to which semantic meanings of שכב. Valency tracks the interaction between semantics and syntax.<br>\n",
    "<br>\n",
    "In Dyk et al. they suggest that few \"watertight\" methods exist to separate obligatory, complement functions from non-obligatory, adjunctive functions. (see [Dyk, Glanz, Oosting, \"Analysing Valence Patterns,\"](https://shebanq.ancient-data.org/shebanq/static/docs/methods/2014_Dyk_jnsl.pdf) 4-5). They apply a \"distributional method\" as follows:\n",
    "\n",
    "* \"Collect all occurrences of a verb with the complete patterns of elements occurring in the data.\"\n",
    "* \"Sort these by pattern.\"\n",
    "* \"Analyse the differences between the various patterns, observing what relation the separate sentence constituents have to the verb.\" *(Dyk et al., 6)*\n",
    "\n",
    "Which elements to use? Dyk et al. use:\n",
    "* \"predicate (Pred), subject (Subj), object (Objc), complement (Cmpl), adjunct (Adju).\" (7)\n",
    "\n",
    "The valence corrections notebook ([here](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/corr_enrich.html)) contains good information on procedure. Certain functions are considered \"core,\" i.e., many of the functions above.<br><br>\n",
    "Let's begin by applying the simplest measures first. We'll keep track of how many examples of the predicate we've accounted for as we work from simpler to more complex patterns. \n",
    "<br><br>\n",
    "**Here's the objectives:**<br>\n",
    "1. Inventory phrase functions for relevant phrase functions; organise by these groups.\n",
    "    * This part establishes the valency type of the verb. Is it transitive or intransitive? Monovalent, divalent, or trivalent? Are there examples of valence expansion or valence reduction?\n",
    "2. Further subdivide the general patterns with semantic/lexical distinctions\n",
    "    * Roorda & Dyk's valency correction notebook (above) further sub-categorises functions based on semantic distinctions such as location, time, instrumentality, and a few more. The database as-is does not contain these distinctions. But some may be inferred from the [features](https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/0_overview.html): \n",
    "        * **`nametype`**\n",
    "        * **`gloss`** (used in conjunction with a resource like [WordNet](http://www.nltk.org/howto/wordnet.html) or [FrameNet](http://www.nltk.org/howto/framenet.html) through the NLTK package)\n",
    "        * **`uvf`** (for ה locative markers).\n",
    "    * Perhaps also different prepositions might give rise to different senses?\n",
    "\n",
    "Procedural question: should the order of elements matter? For the time being, let's keep things simple by ignoring the order of elements. This is something that can be analysed secondarily. Or we can go back if necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.0.0\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/0_overview.html\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "106 features found and 0 ignored\n",
      "\n",
      "  0.00s loading features ...\n",
      "   |     0.04s B otype                from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B book                 from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B chapter              from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B verse                from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.19s B g_cons               from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.24s B g_cons_utf8          from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.10s B function             from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.17s B pdp                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.17s B sp                   from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.17s B vs                   from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.18s B lex                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.00s B nametype             from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.16s B ls                   from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B gloss                from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.18s B uvf                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.00s Feature overview: 101 nodes; 4 edges; 1 configs; 6 computeds\n",
      "  7.28s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "import collections as col\n",
    "from tf.fabric import Fabric\n",
    "\n",
    "TF = Fabric(modules='Hebrew/etcbc4c')\n",
    "print()\n",
    "api = TF.load(\"\"\"otype\n",
    "                 book chapter verse\n",
    "                 function pdp sp vs\n",
    "                 lex g_cons g_cons_utf8\n",
    "                 nametype ls gloss uvf\n",
    "                \"\"\")\n",
    "\n",
    "api.makeAvailableIn(globals()) # so we don't have to say api.F.feature.v() but only F.feature.v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "instances found:  195\n",
      "avg. # of satellites:  2.24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# collect all clauses that contain the target verb CKB with a phrase function of predicate\n",
    "\n",
    "target = 'CKB['\n",
    "stem = 'qal' # we are only studying the qal stem for now\n",
    "\n",
    "# derived from valency corrections notebook (Roorda&Dyk)\n",
    "# for now we take only functions that have a regular verb \n",
    "predicate_functions = {'Pred', 'PreS', 'PreO', 'PreC', 'PtcO', 'PrcS'} \n",
    "# complements | adjuncts:\n",
    "cmpl_adj = {'Cmpl','Adju','Time','Supp', 'Objc','Subj','ModS','NCoS','IntS','PrAd'} \n",
    "\n",
    "def find_satellites(target, stem, pred_functions, comp_adj):\n",
    "    '''\n",
    "    takes a lemma&stem and returns a dict containing:\n",
    "    results[clause_atom_node] = [phrase_function for pf in relevant_phrase_functions]\n",
    "    '''\n",
    "    satellites = dict()\n",
    "    for word in F.otype.s('word'):\n",
    "        lex = F.lex.v(word)\n",
    "        if lex != target:\n",
    "            continue\n",
    "        phrase_node = L.u(word, otype='phrase')[0]\n",
    "        phrase_func = F.function.v(phrase_node)\n",
    "        if phrase_func not in pred_functions or F.vs.v(word) != stem:\n",
    "            continue\n",
    "        clause_node = L.u(phrase_node, otype = 'clause_atom')[0]\n",
    "        phrase_nodeS = L.d(clause_node, otype = 'phrase')\n",
    "        phrase_functs = list(F.function.v(phrase) for phrase in phrase_nodeS if F.function.v(phrase)\\\n",
    "                             in pred_functions | comp_adj)\n",
    "        satellites[clause_node] = phrase_functs\n",
    "    return satellites\n",
    "\n",
    "ckb_sats = find_satellites(target, stem, predicate_functions, cmpl_adj)\n",
    "\n",
    "# display the average length of the gathered data\n",
    "def avg_data(data_dictionary):\n",
    "    total_datPoints = len(data_dictionary.values())\n",
    "    total_datLengths = sum(len(datpoint) for datpoint in data_dictionary.values())\n",
    "    return round(total_datLengths / total_datPoints, 2)\n",
    "\n",
    "print()\n",
    "print('instances found: ', len(ckb_sats))\n",
    "print('avg. # of satellites: ', avg_data(ckb_sats))        \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inventory of all registered functions for CKB:\n",
      "{'Time', 'Cmpl', 'PreC', 'Objc', 'PreO', 'IntS', 'Subj', 'Pred', 'PreS', 'Adju'}\n"
     ]
    }
   ],
   "source": [
    "print('Inventory of all registered functions for CKB:')\n",
    "print(set(function for function_list in ckb_sats.values() for function in function_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the elements in the function codes above are superfluous or unnecessarily connected. For example: `PreS` with a suffixed subject belongs in the same category as `Pred+Subj`. In the `IntS` function, the interrogative is (at this point) superfluous for phrase-level valency function, but the `S` of subject is necessary. Let's simplify those labels. While we're at it, we'll convert the codes into more readable forms and also convert the `Objc` label into a direct object label.<br><br>\n",
    "We keep the copy of the original dictionary in case we find that the altered labels contain valuable data later during the analysis stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OLD average num. of satellites per instance:\n",
      "2.24 \n",
      "\n",
      "New average num. of satellites per instance:\n",
      "2.41\n"
     ]
    }
   ],
   "source": [
    "# the new labels will be attached to a string and .split()'ed:\n",
    "func_convert = {'Subj' : 'subject',    \n",
    "                'PreC' : 'predicate complement',\n",
    "                'Adju' : 'adjunct',\n",
    "                'PreO' : 'predicate direct_object',\n",
    "                'Pred' : 'predicate',\n",
    "                'IntS' : 'subject',\n",
    "                'PreS' : 'predicate subject',\n",
    "                'Objc' : 'direct_object',\n",
    "                'Time' : 'adjunct', # cf. Roorda&Dyk notebook\n",
    "                'Cmpl' : 'complement'\n",
    "                }\n",
    "\n",
    "simple_ckb_sats = dict()\n",
    "\n",
    "for instance, satellites in ckb_sats.items():\n",
    "    satellites = ' '.join(func_convert[fn] for fn in satellites)\n",
    "    simple_ckb_sats[instance] = satellites.split()\n",
    "    \n",
    "print('\\nOLD average num. of satellites per instance:')\n",
    "print(avg_data(ckb_sats),'\\n')\n",
    "print('New average num. of satellites per instance:')\n",
    "print(avg_data(simple_ckb_sats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ a higher avg. means we've succeeded in splitting several of the combined satellites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to present some data...<br>\n",
    "I'll be using some HTML tricks inspired by [Gino Kalkman's notebook](https://github.com/ETCBC/Biblical_Hebrew_Analysis/blob/master/Miscellaneous/AsyndeticClauseFunctions.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's write some functions for displaying some statistics:\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def generate_table(fieldnames, data, style=''):\n",
    "    '''\n",
    "    returns HTML table when fed a fieldnames list and list of tuples in order\n",
    "    optional argument to configure text style\n",
    "    '''\n",
    "    table_code = '<table>'     # here is where all the code will be assembled\n",
    "    table_row = '<tr>{}</tr>'  # template for table rows\n",
    "    # assemble table_header\n",
    "    table_header = ''\n",
    "    for fieldname in fieldnames:\n",
    "        table_header += '<th{style}>{header}</th>'.format(style=style, \n",
    "                                                          header=fieldname)\n",
    "    # assemble table_rows\n",
    "    table_rows = ''\n",
    "    for data_tuple in data:\n",
    "        row = ''\n",
    "        for data in data_tuple:    \n",
    "            row += '<td{style}>{data}</td>'.format(style='',\n",
    "                                                   data=data)\n",
    "        table_rows += table_row.format(row)\n",
    "    # complete the code:\n",
    "    table_code += table_row.format(table_header)\n",
    "    table_code += table_row.format(table_rows)\n",
    "    table_code += '</table>'\n",
    "    # display the code\n",
    "    display(HTML(table_code))\n",
    "    \n",
    "def percent(amount, total):\n",
    "    '''\n",
    "    return a simple percentage\n",
    "    '''\n",
    "    return round((amount/total)*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th style=\"text-align:center\">Total</th><th style=\"text-align:center\"> % </th><th style=\"text-align:center\">Pattern</th></tr><tr><tr><td>50</td><td>25.64</td><td>('complement', 'predicate')</td></tr><tr><td>43</td><td>22.05</td><td>('complement', 'predicate', 'subject')</td></tr><tr><td>25</td><td>12.82</td><td>('predicate',)</td></tr><tr><td>13</td><td>6.67</td><td>('predicate', 'subject')</td></tr><tr><td>12</td><td>6.15</td><td>('adjunct', 'predicate')</td></tr><tr><td>9</td><td>4.62</td><td>('complement', 'complement', 'predicate', 'subject')</td></tr><tr><td>9</td><td>4.62</td><td>('complement', 'complement', 'predicate')</td></tr><tr><td>7</td><td>3.59</td><td>('adjunct', 'complement', 'predicate')</td></tr><tr><td>7</td><td>3.59</td><td>('direct_object', 'predicate')</td></tr><tr><td>5</td><td>2.56</td><td>('adjunct', 'predicate', 'subject')</td></tr><tr><td>4</td><td>2.05</td><td>('adjunct', 'direct_object', 'predicate')</td></tr><tr><td>3</td><td>1.54</td><td>('direct_object', 'predicate', 'subject')</td></tr><tr><td>3</td><td>1.54</td><td>('adjunct', 'complement', 'predicate', 'subject')</td></tr><tr><td>2</td><td>1.03</td><td>('direct_object', 'direct_object', 'predicate', 'subject')</td></tr><tr><td>1</td><td>0.51</td><td>('complement', 'predicate', 'predicate')</td></tr><tr><td>1</td><td>0.51</td><td>('adjunct', 'adjunct', 'complement', 'predicate')</td></tr><tr><td>1</td><td>0.51</td><td>('complement', 'direct_object', 'predicate', 'subject')</td></tr></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# assemble some stats to display:\n",
    "ckb_simple_sTats = col.Counter()\n",
    "for instance, pattern in simple_ckb_sats.items():\n",
    "    ckb_simple_sTats[ tuple(sorted(pattern)) ] += 1\n",
    "    \n",
    "# -- data for the HTML viewer -- \n",
    "\n",
    "fieldnames = ['Total',' % ', 'Pattern']\n",
    "all_patts = sum(ckb_simple_sTats.values())\n",
    "# a list of dicts with fieldname keys:\n",
    "ckb_simple_data = list( (total, percent(total, all_patts), pattern)\\\n",
    "                       for pattern, total in sorted(ckb_simple_sTats.items(), key = lambda k: -k[1])\n",
    "                      ) \n",
    "# display table:\n",
    "print()\n",
    "generate_table(fieldnames, ckb_simple_data, style=' style=\"text-align:center\"')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note that the elements in these patterns are sorted alphabetically, not in the order of occurrence.<br><br>\n",
    "\n",
    "Some first observations:<br>\n",
    "* ~ **`53.84%`** of the patterns have only\n",
    "    * 1 complement or\n",
    "    * 1 adjunct\n",
    "    \n",
    "    \n",
    "* ~ **`19.49%`** with\n",
    "    * 0 other elements except for a subject\n",
    "    \n",
    "    \n",
    "* ~ **`5.13%`** with\n",
    "    * 1 direct object\n",
    "    \n",
    "The rest of the patterns have complex mixtures of complements, adjuncts, and objects as well as doubles.<br><br>\n",
    "\n",
    "Perhaps there are some further refinements we can apply to the categories. It's not certain whether the presence of a subject is relevant for the phrase functions. But it's impossible to know until the examples can be manually sorted. Before we move to that stage, we'll add some more information about the lexical and semantic qualities (and subcategories) of the groups observed above.\n",
    "\n",
    "## Step 2: Lexical and Semantic Categories\n",
    "\n",
    "We now have some basic groups and information to build further queries upon. This next step entails measuring the lexical and semantic qualities of the שכב satellites. Some features we're looking for:\n",
    "* locative lexemes - lexemes that imply spatial distinctions, and thus movement\n",
    "* agentive lexemes - lexemes that imply reception of the action\n",
    "* instrumental lexemes - lexemes that imply the use of objects/tools in the action\n",
    "\n",
    "Further subclassifications may break down by preposition use. See, for example, the activity in the Roorda&Dyk corrections notebook which contain ל and כ objects. That notebook already contains some rules defined for L/K objects:\n",
    "* \"start with either preposition L or K and\n",
    "* the L or K in question does not carry a pronominal suffix\n",
    "* should also not be followed by a body part\" \n",
    "([Roorda&Dyk](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/corr_enrich.html), \"Complements as LK Objects\")\n",
    "\n",
    "Since these kinds of features are not presently stored in ETCBC4c, we have to use a few tricks to procure them. In some cases, items considered \"adjuncts\" (i.e., \"unnecessary\"), will need to be reconsidered as complements (key to the semantic meaning). **Locative** and **agentive** lexemes will be facilitated by the features:\n",
    "* `nametype`\n",
    "    * = `topo` (place)\n",
    "    * = `pers` (person)\n",
    "    * = `gens` (people)\n",
    "* `uvf`\n",
    "    * contains locative ה\n",
    "* `ls`\n",
    "    * = `gentilic` (i.e. demonym)\n",
    "    \n",
    "I also would like to experiment with deploying Framenet or Wordnet combined with the `gloss` feature as a method of categorising lexemes. This may prove especially useful for **instrumental** terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_1 -  111 members\n",
      "('complement', 'predicate', 'predicate')\n",
      "('complement', 'predicate')\n",
      "('adjunct', 'predicate', 'subject')\n",
      "('adjunct', 'predicate')\n",
      "('complement', 'predicate', 'subject')\n",
      "\n",
      "group_2 -  10 members\n",
      "('direct_object', 'predicate', 'subject')\n",
      "('direct_object', 'predicate')\n",
      "\n",
      "group_3 -  38 members\n",
      "('predicate',)\n",
      "('predicate', 'subject')\n",
      "\n",
      "group_4 -  36 members\n",
      "('adjunct', 'complement', 'predicate')\n",
      "('adjunct', 'direct_object', 'predicate')\n",
      "('complement', 'complement', 'predicate', 'subject')\n",
      "('direct_object', 'direct_object', 'predicate', 'subject')\n",
      "('adjunct', 'adjunct', 'complement', 'predicate')\n",
      "('complement', 'direct_object', 'predicate', 'subject')\n",
      "('adjunct', 'complement', 'predicate', 'subject')\n",
      "('complement', 'complement', 'predicate')\n",
      "\n",
      "Group_all 195\n"
     ]
    }
   ],
   "source": [
    "# First we need to organise the 4 simple groups we observed above.\n",
    "# We treat group_4, mixed satellites, as a catch-all for now,\n",
    "#   so that we can deal with the simpler structures first.\n",
    "\n",
    "basic_groups = col.defaultdict(set)       # a dict keyed by group, valued by sets of clause nodes\n",
    "basic_groups_check = col.defaultdict(set) # to double-check our results\n",
    "\n",
    "# rules for groups based on counts:\n",
    "         # (adj, compl, d.o.)\n",
    "group_rules = { (1,0,0) : 'group_1', # 1 adj ø else\n",
    "                (0,1,0) : 'group_1', # 1 comp ø else\n",
    "                (0,0,1) : 'group_2', # 1 d.o. ø else\n",
    "                (0,0,0) : 'group_3', # ø else\n",
    "               #all else : group_4\n",
    "              }\n",
    "\n",
    "for clause_node, pattern in simple_ckb_sats.items():\n",
    "    adj_count = pattern.count('adjunct')\n",
    "    comp_count = pattern.count('complement')\n",
    "    do_count = pattern.count('direct_object')\n",
    "    count = (adj_count,comp_count,do_count) \n",
    "    if count in group_rules:\n",
    "        group = group_rules[count]\n",
    "    else:\n",
    "        group = 'group_4'\n",
    "    basic_groups[group].add(clause_node)\n",
    "    basic_groups_check[group].add(tuple(sorted(pattern)))\n",
    "        \n",
    "# -- Double Check Our Results -- #\n",
    "        \n",
    "total_check = 0\n",
    "for group, group_nodes in sorted(basic_groups.items()):\n",
    "    total_check += len(group_nodes)\n",
    "    print(group+' - ', len(group_nodes), 'members')\n",
    "    for patt in basic_groups_check[group]:\n",
    "        print(patt)\n",
    "    print()\n",
    "print('Group_all', total_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good. We can move on to measuring semantics throughout each group to create more nuanced categories.\n",
    "\n",
    "The key to these queries is the **noun**, upon which we'll perform checks for semantic data. The Roorda&Dyk notebook contains a nice example of measuring semantic ideas with a scoring system. I'll keep this in mind as I move ahead...\n",
    "\n",
    "First, we work with group 1 and inventory the patterns in the complement phrases. The inventory will provide a basis for the semantic work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Total</th><th>Pattern</th></tr><tr><tr><td>57</td><td>('prep', 'subs')</td></tr><tr><td>19</td><td>('prep',)</td></tr><tr><td>6</td><td>('prep', 'art', 'subs')</td></tr><tr><td>4</td><td>('verb',)</td></tr><tr><td>4</td><td>('advb',)</td></tr><tr><td>4</td><td>('subs',)</td></tr><tr><td>2</td><td>('prep', 'subs', 'art', 'subs')</td></tr><tr><td>2</td><td>('prep', 'subs', 'prep', 'subs', 'subs')</td></tr><tr><td>2</td><td>('prep', 'subs', 'subs')</td></tr><tr><td>2</td><td>('prep', 'subs', 'art', 'adjv')</td></tr><tr><td>2</td><td>('prep', 'adjv', 'subs')</td></tr><tr><td>1</td><td>('prep', 'prin')</td></tr><tr><td>1</td><td>('prep', 'subs', 'nmpr')</td></tr><tr><td>1</td><td>('prep', 'subs', 'conj', 'prep', 'subs')</td></tr><tr><td>1</td><td>('advb', 'prep', 'art', 'subs')</td></tr><tr><td>1</td><td>('prep', 'subs', 'prep', 'subs', 'nmpr')</td></tr><tr><td>1</td><td>('prep', 'art', 'subs', 'art', 'prde')</td></tr><tr><td>1</td><td>('prep', 'nmpr', 'subs', 'subs')</td></tr></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GROUP 1 - SINGLE ADJUNCT OR COMPLEMENTS\n",
    "# WORD FUNCTION INVENTORIES\n",
    "# query and inventory word-level, internal phrase functions in group 1\n",
    "\n",
    "group1_phrases = dict()\n",
    "group1_stats = col.Counter()\n",
    "\n",
    "for clause_atom in basic_groups['group_1']:\n",
    "    phrases = L.d(clause_atom, otype='phrase')\n",
    "    target_phrase = None\n",
    "    for phrase in phrases:\n",
    "        func = F.function.v(phrase)\n",
    "        simple_form = func_convert[func].split() if func in func_convert else func\n",
    "        target_phrase = phrase if {'adjunct','complement'} & set(simple_form)\\\n",
    "                               else target_phrase\n",
    "    pattern = tuple(F.pdp.v(word) for word in L.d(target_phrase, otype='word'))\n",
    "    group1_phrases[clause_atom] = {'phrase':target_phrase, 'pattern':pattern}\n",
    "    group1_stats[pattern] += 1\n",
    "    \n",
    "group1_header = ['Total','Pattern']\n",
    "display_group1 = list((a,p) for p, a in sorted(group1_stats.items(), key=lambda k: -k[1]))\n",
    "\n",
    "generate_table(group1_header, display_group1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show the prominent position of the preposition in group 1 constructions.\n",
    "\n",
    "57 of the results contain only a preposition and a substantive. We'll focus first on this simpler form. Hopefully this will give a basis on which to process the rarer, more complex examples.\n",
    "\n",
    "### Sense Generator\n",
    "We're building a sense generator that will return 1 of 3 categories for a given lemma. The categories are:\n",
    "* person\n",
    "* place\n",
    "* thing/object\n",
    "\n",
    "The machine will use 4 sources to make its decisions: \n",
    "1. existing features in the ETCBC\n",
    "2. lists of categorised lexemes from the [Roorda/Dyk notebook](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/corr_enrich.html)\n",
    "3. 3 and 4 are special side-projects that are in progress:\n",
    "    * see [valency_wordlists](https://github.com/codykingham/textfabric_notebooks/blob/master/valency_wordlists)\n",
    "    * (3) generated category lists from [openscriptures' BDB lexicon](https://github.com/openscriptures/HebrewLexicon) using part-of-speech tags in BDB\n",
    "    * (4) generated category lists from Wordnet, using hypernym relations\n",
    "\n",
    "Each of the sources will count as 1 or more parameters, that, if fulfilled, will go towards a score for the given lexeme object. The categorisation is based on a simple majority, but scores can also be returned with a \"strength\", for example, 3/3 or 2/3, depending on how many parameters are met out of how many are applicable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ('prep', 'subs'), 57 examples\n",
    "# LOCATIVITY - how many of these examples have a locative substantive as the obj. of the prep.?\n",
    "\n",
    "#nametype\n",
    "    #= topo (place)\n",
    "    #= pers (person)\n",
    "    #= gens (people)\n",
    "#uvf\n",
    "    #contains locative ה\n",
    "#ls\n",
    "    #= gentilic (i.e. demonym)\n",
    "\n",
    "def test_locative(word_node):\n",
    "    # SOURCE 1, ETCBC4c\n",
    "    lex_obj = L.u(word_node, otype = 'lex')[0]\n",
    "    univalent_final = F.uvf.v(word_node) if F.uvf.v(word_node) == 'H' else None\n",
    "    name_type = F.nametype.v(lex_obj) if F.nametype.v(lex_obj) == 'topo' else None\n",
    "    s1_score = len([s for s in (univalent_final, name_type) if s])\n",
    "    # SOURCE 2, ROORDA/DYK\n",
    "    lexeme = F.lex.v(word_node)\n",
    "    locative_lexs = set('''\n",
    "                            >RY/ >YL/ >XR/<BR/ <BRH/ <BWR/ <C==/ <JR/ <L=/ \n",
    "                            <LJ=/ <LJH/ <LJL/ <MD=/ <MDH/ <MH/ <MQ/ <MQ===/ \n",
    "                            <QB/ BJN/ BJT/ CM CMJM/ CMC/ C<R/ DRK/ FDH/ HR/\n",
    "                            JM/ JRDN/ JRWCLM/ JFR>L/ MDBR/ MW<D/ MWL/ MZBX/ \n",
    "                            MYRJM/ MQWM/ MR>CWT/ MSB/ MSBH/ MVH==/ QDM/\n",
    "                            SBJB/ TJMN/ TXT/ TXWT/ YPWN/\n",
    "                            '''.strip().split())\n",
    "    s2_score = len({lexeme}&locative_lexs)\n",
    "    \n",
    "    # SOURCE 3, BDB categories list\n",
    "    # source 4, wordnet categories list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
