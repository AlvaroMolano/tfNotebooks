{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `שכב` valency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook last modified on 2016-12-31 17:03:37.927905\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "last_modified = datetime.now()\n",
    "print('Notebook last modified on {}'.format(last_modified.__str__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Methodology\n",
    "The goal is to inventory and categorise the various satellites surrounding the verb שכב (\"to lie\") in biblical Hebrew in order to determine which elements give rise to which semantic meanings of שכב. Valency tracks the interaction between semantics and syntax.<br>\n",
    "<br>\n",
    "In Dyk et al. they suggest that few \"watertight\" methods exist to separate obligatory, complement functions from non-obligatory, adjunctive functions. (see [Dyk, Glanz, Oosting, \"Analysing Valence Patterns,\"](https://shebanq.ancient-data.org/shebanq/static/docs/methods/2014_Dyk_jnsl.pdf) 4-5). They apply a \"distributional method\" as follows:\n",
    "\n",
    "* \"Collect all occurrences of a verb with the complete patterns of elements occurring in the data.\"\n",
    "* \"Sort these by pattern.\"\n",
    "* \"Analyse the differences between the various patterns, observing what relation the separate sentence constituents have to the verb.\" *(Dyk et al., 6)*\n",
    "\n",
    "Which elements to use? Dyk et al. use:\n",
    "* \"predicate (Pred), subject (Subj), object (Objc), complement (Cmpl), adjunct (Adju).\" (7)\n",
    "\n",
    "The valence corrections notebook ([here](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/corr_enrich.html)) contains good information on procedure. Certain functions are considered \"core,\" i.e., many of the functions above.<br><br>\n",
    "Let's begin by applying the simplest measures first. We'll keep track of how many examples of the predicate we've accounted for as we work from simpler to more complex patterns. \n",
    "<br><br>\n",
    "**Here's the objectives:**<br>\n",
    "1. Inventory phrase functions for relevant phrase functions; organise by these groups.\n",
    "    * This part establishes the valency type of the verb. Is it transitive or intransitive? Monovalent, divalent, or trivalent? Are there examples of valence expansion or valence reduction?\n",
    "2. Further subdivide the general patterns with semantic/lexical distinctions\n",
    "    * Roorda & Dyk's valency correction notebook (above) further sub-categorises functions based on semantic distinctions such as location, time, instrumentality, and a few more. The database as-is does not contain these distinctions. But some may be inferred from the [features](https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/0_overview.html): \n",
    "        * **`nametype`**\n",
    "        * **`gloss`** (used in conjunction with a resource like [WordNet](http://www.nltk.org/howto/wordnet.html) or [FrameNet](http://www.nltk.org/howto/framenet.html) through the NLTK package)\n",
    "        * **`uvf`** (for ה locative markers).\n",
    "    * Perhaps also different prepositions might give rise to different senses?\n",
    "\n",
    "Procedural question: should the order of elements matter? For the time being, let's keep things simple by ignoring the order of elements. This is something that can be analysed secondarily. Or we can go back if necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.0.0\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/0_overview.html\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "106 features found and 0 ignored\n",
      "\n",
      "  0.00s loading features ...\n",
      "   |     0.03s B otype                from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B book                 from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B chapter              from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B verse                from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.21s B g_cons               from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.24s B g_cons_utf8          from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.10s B function             from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.21s B pdp                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.17s B sp                   from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.16s B vs                   from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.22s B lex                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.00s B nametype             from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.17s B ls                   from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B gloss                from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.19s B uvf                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.00s Feature overview: 101 nodes; 4 edges; 1 configs; 6 computeds\n",
      "  7.24s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "import collections as col\n",
    "from tf.fabric import Fabric\n",
    "\n",
    "TF = Fabric(modules='Hebrew/etcbc4c')\n",
    "print()\n",
    "api = TF.load(\"\"\"otype\n",
    "                 book chapter verse\n",
    "                 function pdp sp vs\n",
    "                 lex g_cons g_cons_utf8\n",
    "                 nametype ls gloss uvf\n",
    "                \"\"\")\n",
    "\n",
    "api.makeAvailableIn(globals()) # so we don't have to say api.F.feature.v() but only F.feature.v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "instances found:  195\n",
      "avg. # of satellites:  2.24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# collect all clauses that contain the target verb CKB with a phrase function of predicate\n",
    "\n",
    "target = 'CKB['\n",
    "stem = 'qal' # we are only studying the qal stem for now\n",
    "\n",
    "# derived from valency corrections notebook (Roorda&Dyk)\n",
    "# for now we take only functions that have a regular verb \n",
    "predicate_functions = {'Pred', 'PreS', 'PreO', 'PreC', 'PtcO', 'PrcS'} \n",
    "# complements | adjuncts:\n",
    "cmpl_adj = {'Cmpl','Adju','Time','Supp', 'Objc','Subj','ModS','NCoS','IntS','PrAd'} \n",
    "\n",
    "def find_satellites(target, stem, pred_functions, comp_adj):\n",
    "    '''\n",
    "    takes a lemma&stem and returns a dict containing:\n",
    "    results[clause_atom_node] = [phrase_function for pf in relevant_phrase_functions]\n",
    "    '''\n",
    "    satellites = dict()\n",
    "    for word in F.otype.s('word'):\n",
    "        lex = F.lex.v(word)\n",
    "        if lex != target:\n",
    "            continue\n",
    "        phrase_node = L.u(word, otype='phrase')[0]\n",
    "        phrase_func = F.function.v(phrase_node)\n",
    "        if phrase_func not in pred_functions or F.vs.v(word) != stem:\n",
    "            continue\n",
    "        clause_node = L.u(phrase_node, otype = 'clause_atom')[0]\n",
    "        phrase_nodeS = L.d(clause_node, otype = 'phrase')\n",
    "        phrase_functs = list(F.function.v(phrase) for phrase in phrase_nodeS if F.function.v(phrase)\\\n",
    "                             in pred_functions | comp_adj)\n",
    "        satellites[clause_node] = phrase_functs\n",
    "    return satellites\n",
    "\n",
    "ckb_sats = find_satellites(target, stem, predicate_functions, cmpl_adj)\n",
    "\n",
    "# display the average length of the gathered data\n",
    "def avg_data(data_dictionary):\n",
    "    total_datPoints = len(data_dictionary.values())\n",
    "    total_datLengths = sum(len(datpoint) for datpoint in data_dictionary.values())\n",
    "    return round(total_datLengths / total_datPoints, 2)\n",
    "\n",
    "print()\n",
    "print('instances found: ', len(ckb_sats))\n",
    "print('avg. # of satellites: ', avg_data(ckb_sats))        \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inventory of all registered functions for CKB:\n",
      "{'Subj', 'Time', 'PreO', 'PreS', 'Cmpl', 'Pred', 'Adju', 'Objc', 'IntS', 'PreC'}\n"
     ]
    }
   ],
   "source": [
    "print('Inventory of all registered functions for CKB:')\n",
    "print(set(function for function_list in ckb_sats.values() for function in function_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the elements in the function codes above are superfluous or unnecessarily connected. For example: `PreS` with a suffixed subject belongs in the same category as `Pred+Subj`. In the `IntS` function, the interrogative is (at this point) superfluous for phrase-level valency function, but the `S` of subject is necessary. Let's simplify those labels. While we're at it, we'll convert the codes into more readable forms and also convert the `Objc` label into a direct object label.<br><br>\n",
    "We keep the copy of the original dictionary in case we find that the altered labels contain valuable data later during the analysis stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OLD average num. of satellites per instance:\n",
      "2.24 \n",
      "\n",
      "New average num. of satellites per instance:\n",
      "2.41\n"
     ]
    }
   ],
   "source": [
    "# the new labels will be attached to a string and .split()'ed:\n",
    "func_convert = {'Subj' : 'subject',    \n",
    "                'PreC' : 'predicate complement',\n",
    "                'Adju' : 'adjunct',\n",
    "                'PreO' : 'predicate direct_object',\n",
    "                'Pred' : 'predicate',\n",
    "                'IntS' : 'subject',\n",
    "                'PreS' : 'predicate subject',\n",
    "                'Objc' : 'direct_object',\n",
    "                'Time' : 'adjunct', # cf. Roorda&Dyk notebook\n",
    "                'Cmpl' : 'complement'\n",
    "                }\n",
    "\n",
    "simple_ckb_sats = dict()\n",
    "\n",
    "for instance, satellites in ckb_sats.items():\n",
    "    satellites = ' '.join(func_convert[fn] for fn in satellites)\n",
    "    simple_ckb_sats[instance] = satellites.split()\n",
    "    \n",
    "print('\\nOLD average num. of satellites per instance:')\n",
    "print(avg_data(ckb_sats),'\\n')\n",
    "print('New average num. of satellites per instance:')\n",
    "print(avg_data(simple_ckb_sats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ a higher avg. means we've succeeded in splitting several of the combined satellites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to present some data...<br>\n",
    "I'll be using some HTML tricks inspired by [Gino Kalkman's notebook](https://github.com/ETCBC/Biblical_Hebrew_Analysis/blob/master/Miscellaneous/AsyndeticClauseFunctions.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's write some functions for displaying some statistics:\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def generate_table(fieldnames, data, style=''):\n",
    "    '''\n",
    "    returns an HTML table when fed a fieldnames list and list of dicts with fieldname keys\n",
    "    optional argument to configure text style\n",
    "    follows similar structure to csv dictwriter args\n",
    "    '''\n",
    "    column_amount = len(fieldnames)    \n",
    "    table_code = '<table>'        # here is where all the code will be assembled\n",
    "    table_row = '<tr>{data}</tr>' # template for table rows\n",
    "    # assemble table_header\n",
    "    table_header = ''\n",
    "    for fieldname in fieldnames:\n",
    "        table_header += '<th{style}>{header}</th>'.format(header = fieldname, style = style)\n",
    "    # assemble table_data\n",
    "    table_data = ''\n",
    "    for data_dict in data:\n",
    "        row = ''\n",
    "        for fieldname in fieldnames:         # use order of the fieldnames\n",
    "            tab_data = data_dict[fieldname]  # the data_dicts are keyed with fieldnames\n",
    "            row += '<td{style}>{data}</td>'.format(style = '', data = tab_data)\n",
    "        table_data += table_row.format(data=row)\n",
    "    # complete the code:\n",
    "    table_code += table_row.format(data = table_header)\n",
    "    table_code += table_row.format(data = table_data)\n",
    "    table_code += '</table>'\n",
    "    # display the code\n",
    "    display(HTML(table_code))\n",
    "    \n",
    "def percent(amount, total):\n",
    "    '''\n",
    "    return a simple percentage\n",
    "    '''\n",
    "    return round((amount/total)*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th style=\"text-align:center\">Total</th><th style=\"text-align:center\"> % </th><th style=\"text-align:center\">Pattern</th></tr><tr><tr><td>50</td><td>25.64</td><td>('complement', 'predicate')</td></tr><tr><td>43</td><td>22.05</td><td>('complement', 'predicate', 'subject')</td></tr><tr><td>25</td><td>12.82</td><td>('predicate',)</td></tr><tr><td>13</td><td>6.67</td><td>('predicate', 'subject')</td></tr><tr><td>12</td><td>6.15</td><td>('adjunct', 'predicate')</td></tr><tr><td>9</td><td>4.62</td><td>('complement', 'complement', 'predicate', 'subject')</td></tr><tr><td>9</td><td>4.62</td><td>('complement', 'complement', 'predicate')</td></tr><tr><td>7</td><td>3.59</td><td>('direct_object', 'predicate')</td></tr><tr><td>7</td><td>3.59</td><td>('adjunct', 'complement', 'predicate')</td></tr><tr><td>5</td><td>2.56</td><td>('adjunct', 'predicate', 'subject')</td></tr><tr><td>4</td><td>2.05</td><td>('adjunct', 'direct_object', 'predicate')</td></tr><tr><td>3</td><td>1.54</td><td>('adjunct', 'complement', 'predicate', 'subject')</td></tr><tr><td>3</td><td>1.54</td><td>('direct_object', 'predicate', 'subject')</td></tr><tr><td>2</td><td>1.03</td><td>('direct_object', 'direct_object', 'predicate', 'subject')</td></tr><tr><td>1</td><td>0.51</td><td>('complement', 'predicate', 'predicate')</td></tr><tr><td>1</td><td>0.51</td><td>('complement', 'direct_object', 'predicate', 'subject')</td></tr><tr><td>1</td><td>0.51</td><td>('adjunct', 'adjunct', 'complement', 'predicate')</td></tr></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# assemble some stats to display:\n",
    "ckb_simple_sTats = col.Counter()\n",
    "for instance, pattern in simple_ckb_sats.items():\n",
    "    ckb_simple_sTats[ tuple(sorted(pattern)) ] += 1\n",
    "    \n",
    "# -- data for the HTML viewer -- \n",
    "\n",
    "fieldnames = ['Total',' % ', 'Pattern']\n",
    "all_patts = sum(ckb_simple_sTats.values())\n",
    "# a list of dicts with fieldname keys:\n",
    "ckb_simple_data = list( {'Total' : total, \n",
    "                         ' % ': percent(total, all_patts),\n",
    "                         'Pattern' : pattern}\\\n",
    "                       for pattern, total in sorted(ckb_simple_sTats.items(), key = lambda k: -k[1])\n",
    "                      ) \n",
    "# display table:\n",
    "print()\n",
    "generate_table(fieldnames, ckb_simple_data, style=' style=\"text-align:center\"')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note that the elements in these patterns are sorted alphabetically, not in the order of occurrence.<br><br>\n",
    "\n",
    "Some first observations:<br>\n",
    "* ~ **`53.84%`** of the patterns have only\n",
    "    * 1 complement or\n",
    "    * 1 adjunct\n",
    "    \n",
    "    \n",
    "* ~ **`19.49%`** with\n",
    "    * 0 other elements except for a subject\n",
    "    \n",
    "    \n",
    "* ~ **`5.13%`** with\n",
    "    * 1 direct object\n",
    "    \n",
    "The rest of the patterns have complex mixtures of complements, adjuncts, and objects as well as doubles.<br><br>\n",
    "\n",
    "Perhaps there are some further refinements we can apply to the categories. It's not certain whether the presence of a subject is relevant for the phrase functions. But it's impossible to know until the examples can be manually sorted. Before we move to that stage, we'll add some more information about the lexical and semantic qualities (and subcategories) of the groups observed above.\n",
    "\n",
    "## Step 2: Lexical and Semantic Categories\n",
    "\n",
    "We now have some basic groups and information to build further queries upon. This next step entails measuring the lexical and semantic qualities of the שכב satellites. Some features we're looking for:\n",
    "* locative lexemes - lexemes that imply spatial distinctions, and thus movement\n",
    "* agentive lexemes - lexemes that imply reception of the action\n",
    "* instrumental lexemes - lexemes that imply the use of objects/tools in the action\n",
    "\n",
    "Further subclassifications may break down by preposition use. See, for example, the activity in the Roorda&Dyk corrections notebook which contain ל and כ objects. That notebook already contains some rules defined for L/K objects:\n",
    "* \"start with either preposition L or K and\n",
    "* the L or K in question does not carry a pronominal suffix\n",
    "* should also not be followed by a body part\" \n",
    "([Roorda&Dyk](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/corr_enrich.html), \"Complements as LK Objects\")\n",
    "\n",
    "Since these kinds of features are not presently stored in ETCBC4c, we have to use a few tricks to procure them. In some cases, items considered \"adjuncts\" (i.e., \"unnecessary\"), will need to be reconsidered as complements (key to the semantic meaning). **Locative** and **agentive** lexemes will be facilitated by the features:\n",
    "* `nametype`\n",
    "    * = `topo` (place)\n",
    "    * = `pers` (person)\n",
    "    * = `gens` (people)\n",
    "* `uvf`\n",
    "    * contains locative ה\n",
    "* `ls`\n",
    "    * = `gentilic` (i.e. demonym)\n",
    "    \n",
    "I also would like to experiment with deploying Framenet or Wordnet combined with the `gloss` feature as a method of categorising lexemes. This may prove especially useful for **instrumental** terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_1 -  111 members\n",
      "('adjunct', 'predicate')\n",
      "('adjunct', 'predicate', 'subject')\n",
      "('complement', 'predicate', 'subject')\n",
      "('complement', 'predicate', 'predicate')\n",
      "('complement', 'predicate')\n",
      "\n",
      "group_2 -  38 members\n",
      "('predicate', 'subject')\n",
      "('predicate',)\n",
      "\n",
      "group_3 -  46 members\n",
      "('complement', 'complement', 'predicate', 'subject')\n",
      "('adjunct', 'direct_object', 'predicate')\n",
      "('adjunct', 'complement', 'predicate', 'subject')\n",
      "('direct_object', 'predicate')\n",
      "('direct_object', 'direct_object', 'predicate', 'subject')\n",
      "('complement', 'direct_object', 'predicate', 'subject')\n",
      "('complement', 'complement', 'predicate')\n",
      "('adjunct', 'adjunct', 'complement', 'predicate')\n",
      "('adjunct', 'complement', 'predicate')\n",
      "('direct_object', 'predicate', 'subject')\n",
      "\n",
      "Group_all 195\n"
     ]
    }
   ],
   "source": [
    "# First we need to organise the 3 simple groups we observed above: \n",
    "    # 1 complement or adjunct, no complement/adj, and mixed patterns\n",
    "# In this way we will proceed from the simpler examples to the more complex ones (the last group)\n",
    "\n",
    "basic_groups = col.defaultdict(set) # a dict keyed by group, valued by sets of clause nodes\n",
    "basic_groups_check = col.defaultdict(set) # to double-check our results\n",
    "\n",
    "for clause_node, pattern in simple_ckb_sats.items():\n",
    "    # group 1\n",
    "    if all([ pattern.count('adjunct') == 1 or pattern.count('complement') == 1,\n",
    "             pattern.count('adjunct') == 0 or pattern.count('complement') == 0, # one of them must be absent\n",
    "             pattern.count('direct_object') == 0\n",
    "           ]):\n",
    "        basic_groups['group_1'].add(clause_node)\n",
    "        basic_groups_check['group_1'].add(tuple(sorted(pattern)))\n",
    "    #group 2\n",
    "    elif set(pattern) & {'adjunct','complement','direct_object'} == set():\n",
    "        basic_groups['group_2'].add(clause_node)\n",
    "        basic_groups_check['group_2'].add(tuple(sorted(pattern)))\n",
    "    # group 3\n",
    "    else:\n",
    "        basic_groups['group_3'].add(clause_node)\n",
    "        basic_groups_check['group_3'].add(tuple(sorted(pattern)))\n",
    "        \n",
    "# -- Double Check Our Results -- #\n",
    "        \n",
    "total_check = 0\n",
    "for group, group_nodes in sorted(basic_groups.items()):\n",
    "    total_check += len(group_nodes)\n",
    "    print(group+' - ', len(group_nodes), 'members')\n",
    "    for patt in basic_groups_check[group]:\n",
    "        print(patt)\n",
    "    print()\n",
    "print('Group_all', total_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good. We can move on to measuring semantics throughout each group.\n",
    "\n",
    "The key to these queries is the **noun**, upon which we'll perform checks for semantic data. The Roorda&Dyk notebook contains a nice example of measuring semantic ideas with a scoring system.\n",
    "\n",
    "How can I build on that system?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
