{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valency of `שכב`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook last modified on 2016-12-30 00:04:50.108449\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "last_modified = datetime.now()\n",
    "print('Notebook last modified on {}'.format(last_modified.__str__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Methodology\n",
    "The goal is to inventory and categorise the various satellites surrounding the verb שכב (\"to lie\") in biblical Hebrew in order to determine which elements give rise to which semantic meanings of שכב. Valency tracks the interaction between semantics and syntax.<br>\n",
    "<br>\n",
    "In Dyk et al. they suggest that few \"watertight\" methods exist to separate obligatory, complement functions from non-obligatory, adjunctive functions. (see [Dyk, Glanz, Oosting, \"Analysing Valence Patterns,\"](https://shebanq.ancient-data.org/shebanq/static/docs/methods/2014_Dyk_jnsl.pdf) 4-5). They apply a \"distributional method\" as follows:\n",
    "\n",
    "* \"Collect all occurrences of a verb with the complete patterns of elements occurring in the data.\"\n",
    "* \"Sort these by pattern.\"\n",
    "* \"Analyse the differences between the various patterns, observing what relation the separate sentence constituents have to the verb.\" *(Dyk et al., 6)*\n",
    "\n",
    "Which elements to use? Dyk et al. use:\n",
    "* \"predicate (Pred), subject (Subj), object (Objc), complement (Cmpl), adjunct (Adju).\" (7)\n",
    "\n",
    "The valence corrections notebook ([here](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/corr_enrich.html)) contains good information on procedure. Certain functions are considered \"core,\" i.e., many of the functions above.<br><br>\n",
    "Let's begin by applying the simplest measures first. We'll keep track of how many examples of the predicate we've accounted for as we work from simpler to more complex patterns. \n",
    "<br><br>\n",
    "**Here's the objectives:**<br>\n",
    "1. Inventory phrase functions for relevant phrase functions; organise by these groups.\n",
    "    * This part establishes the valency type of the verb. Is it transitive or intransitive? Monovalent, divalent, or trivalent? Are there examples of valence expansion or valence reduction?\n",
    "2. Further subdivide the general patterns with semantic/lexical distinctions\n",
    "    * Roorda & Dyk's valency correction notebook (above) further sub-categorises functions based on semantic distinctions such as location, time, instrumentality, and a few more. The database as-is does not contain these distinctions. But some may be inferred from the [features](https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/0_overview.html): \n",
    "        * **`nametype`**\n",
    "        * **`gloss`** (used in conjunction with a resource like [WordNet](http://www.nltk.org/howto/wordnet.html) or [FrameNet](http://www.nltk.org/howto/framenet.html) through the NLTK package)\n",
    "        * **`uvf`** (for ה locative markers).\n",
    "    * Perhaps also different prepositions might give rise to different senses?\n",
    "\n",
    "Procedural question: should the order of elements matter? For the time being, let's keep things simple by ignoring the order of elements. This is something that can be analysed secondarily. Or we can go back if necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.0.0\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/0_overview.html\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "106 features found and 0 ignored\n",
      "\n",
      "  0.00s loading features ...\n",
      "   |     0.05s B otype                from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B book                 from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B chapter              from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B verse                from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.25s B g_cons               from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.26s B g_cons_utf8          from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.09s B function             from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.17s B pdp                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.15s B vs                   from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.16s B lex                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.00s B nametype             from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B gloss                from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     1.64s T uvf                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.00s Feature overview: 101 nodes; 4 edges; 1 configs; 6 computeds\n",
      "  8.68s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "import collections as col\n",
    "from tf.fabric import Fabric\n",
    "\n",
    "TF = Fabric(modules='Hebrew/etcbc4c')\n",
    "print()\n",
    "api = TF.load(\"\"\"otype\n",
    "                 book chapter verse\n",
    "                 function pdp vs\n",
    "                 lex g_cons g_cons_utf8\n",
    "                 nametype gloss uvf\n",
    "                \"\"\")\n",
    "\n",
    "api.makeAvailableIn(globals()) # so we don't have to say api.F.feature.v() but only F.feature.v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "instances found:  195\n",
      "avg. # of satellites:  2.24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# collect all clauses that contain the target verb CKB with a phrase function of predicate\n",
    "\n",
    "target = 'CKB['\n",
    "stem = 'qal' # we are only studying the qal stem for now\n",
    "\n",
    "# derived from valency corrections notebook (Roorda&Dyk)\n",
    "# for now we take only functions that have a regular verb \n",
    "predicate_functions = {'Pred', 'PreS', 'PreO', 'PreC', 'PtcO', 'PrcS'} \n",
    "# complements | adjuncts:\n",
    "cmpl_adj = {'Cmpl','Adju','Time','Supp', 'Objc','Subj','ModS','NCoS','IntS','PrAd'} \n",
    "\n",
    "def find_satellites(target, stem, pred_functions, comp_adj):\n",
    "    '''\n",
    "    takes a lemma&stem and returns a dict containing:\n",
    "    results[clause_atom_node] = [pf for pf in phrase_functions if pf in \"desired functions\"]\n",
    "    '''\n",
    "    satellites = dict()\n",
    "    for word in F.otype.s('word'):\n",
    "        lex = F.lex.v(word)\n",
    "        if lex != target:\n",
    "            continue\n",
    "        phrase_node = L.u(word, otype='phrase')[0]\n",
    "        phrase_func = F.function.v(phrase_node)\n",
    "        if phrase_func not in pred_functions or F.vs.v(word) != stem:\n",
    "            continue\n",
    "        clause_node = L.u(phrase_node, otype = 'clause_atom')[0]\n",
    "        phrase_functs = list(F.function.v(phrase) for phrase in L.d(clause_node, otype = 'phrase')\\\n",
    "                             if F.function.v(phrase) in pred_functions | comp_adj)\n",
    "        satellites[clause_node] = phrase_functs\n",
    "    return satellites\n",
    "\n",
    "ckb_sats = find_satellites(target, stem, predicate_functions, cmpl_adj)\n",
    "\n",
    "# display the average length of the gathered data\n",
    "def avg_data(data_dictionary):\n",
    "    total_datPoints = len(data_dictionary.values())\n",
    "    total_datLengths = sum(len(datpoint) for datpoint in data_dictionary.values())\n",
    "    return round(total_datLengths / total_datPoints, 2)\n",
    "\n",
    "print()\n",
    "print('instances found: ', len(ckb_sats))\n",
    "print('avg. # of satellites: ', avg_data(ckb_sats))        \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inventory of all registered functions for CKB:\n",
      "{'Subj', 'PreC', 'Adju', 'PreO', 'Pred', 'IntS', 'PreS', 'Objc', 'Time', 'Cmpl'}\n"
     ]
    }
   ],
   "source": [
    "print('Inventory of all registered functions for CKB:')\n",
    "print(set(function for function_list in ckb_sats.values() for function in function_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the elements in the function codes above are superfluous or unnecessarily connected. For example: `PreS` with a suffixed subject belongs in the same category as `Pred+Subj`. In the `IntS` function, the interrogative is (at this point) superfluous for phrase-level valency function, but the `S` of subject is necessary. Let's simplify those labels. While we're at it, we'll convert the codes into more readable forms and also convert the `Objc` label into a direct object label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New average num. of satellites per instance:\n",
      "2.41\n"
     ]
    }
   ],
   "source": [
    "func_convert = {'Subj' : 'subject',    # the new labels will be attached to a string and .split()'ed\n",
    "                'PreC' : 'predicate complement',\n",
    "                'Adju' : 'adjunct',\n",
    "                'PreO' : 'predicate direct_object',\n",
    "                'Pred' : 'predicate',\n",
    "                'IntS' : 'subject',\n",
    "                'PreS' : 'predicate subject',\n",
    "                'Objc' : 'direct_object',\n",
    "                'Time' : 'adjunct', # cf. Roorda&Dyk notebook\n",
    "                'Cmpl' : 'complement'\n",
    "                }\n",
    "\n",
    "for instance, satellites in ckb_sats.items():\n",
    "    satellites = ' '.join(func_convert[fn] for fn in satellites)\n",
    "    ckb_sats[instance] = satellites.split()\n",
    "    \n",
    "print('New average num. of satellites per instance:')\n",
    "print(avg_data(ckb_sats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ a higher avg. means we've succeeded in splitting several of the combined satellites. Now we advance to visualising and examining the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
