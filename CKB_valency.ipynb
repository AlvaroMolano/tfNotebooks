{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `שכב` valency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook last modified on 2016-12-30 03:37:56.865615\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "last_modified = datetime.now()\n",
    "print('Notebook last modified on {}'.format(last_modified.__str__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Methodology\n",
    "The goal is to inventory and categorise the various satellites surrounding the verb שכב (\"to lie\") in biblical Hebrew in order to determine which elements give rise to which semantic meanings of שכב. Valency tracks the interaction between semantics and syntax.<br>\n",
    "<br>\n",
    "In Dyk et al. they suggest that few \"watertight\" methods exist to separate obligatory, complement functions from non-obligatory, adjunctive functions. (see [Dyk, Glanz, Oosting, \"Analysing Valence Patterns,\"](https://shebanq.ancient-data.org/shebanq/static/docs/methods/2014_Dyk_jnsl.pdf) 4-5). They apply a \"distributional method\" as follows:\n",
    "\n",
    "* \"Collect all occurrences of a verb with the complete patterns of elements occurring in the data.\"\n",
    "* \"Sort these by pattern.\"\n",
    "* \"Analyse the differences between the various patterns, observing what relation the separate sentence constituents have to the verb.\" *(Dyk et al., 6)*\n",
    "\n",
    "Which elements to use? Dyk et al. use:\n",
    "* \"predicate (Pred), subject (Subj), object (Objc), complement (Cmpl), adjunct (Adju).\" (7)\n",
    "\n",
    "The valence corrections notebook ([here](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/corr_enrich.html)) contains good information on procedure. Certain functions are considered \"core,\" i.e., many of the functions above.<br><br>\n",
    "Let's begin by applying the simplest measures first. We'll keep track of how many examples of the predicate we've accounted for as we work from simpler to more complex patterns. \n",
    "<br><br>\n",
    "**Here's the objectives:**<br>\n",
    "1. Inventory phrase functions for relevant phrase functions; organise by these groups.\n",
    "    * This part establishes the valency type of the verb. Is it transitive or intransitive? Monovalent, divalent, or trivalent? Are there examples of valence expansion or valence reduction?\n",
    "2. Further subdivide the general patterns with semantic/lexical distinctions\n",
    "    * Roorda & Dyk's valency correction notebook (above) further sub-categorises functions based on semantic distinctions such as location, time, instrumentality, and a few more. The database as-is does not contain these distinctions. But some may be inferred from the [features](https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/0_overview.html): \n",
    "        * **`nametype`**\n",
    "        * **`gloss`** (used in conjunction with a resource like [WordNet](http://www.nltk.org/howto/wordnet.html) or [FrameNet](http://www.nltk.org/howto/framenet.html) through the NLTK package)\n",
    "        * **`uvf`** (for ה locative markers).\n",
    "    * Perhaps also different prepositions might give rise to different senses?\n",
    "\n",
    "Procedural question: should the order of elements matter? For the time being, let's keep things simple by ignoring the order of elements. This is something that can be analysed secondarily. Or we can go back if necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.0.0\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/0_overview.html\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "106 features found and 0 ignored\n",
      "\n",
      "  0.00s loading features ...\n",
      "   |     0.07s B otype                from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B book                 from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B chapter              from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B verse                from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.26s B g_cons               from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.26s B g_cons_utf8          from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.11s B function             from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.20s B pdp                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.17s B vs                   from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.22s B lex                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.00s B nametype             from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B gloss                from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.25s B uvf                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.00s Feature overview: 101 nodes; 4 edges; 1 configs; 6 computeds\n",
      "  7.99s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "import collections as col\n",
    "from tf.fabric import Fabric\n",
    "\n",
    "TF = Fabric(modules='Hebrew/etcbc4c')\n",
    "print()\n",
    "api = TF.load(\"\"\"otype\n",
    "                 book chapter verse\n",
    "                 function pdp vs\n",
    "                 lex g_cons g_cons_utf8\n",
    "                 nametype gloss uvf\n",
    "                \"\"\")\n",
    "\n",
    "api.makeAvailableIn(globals()) # so we don't have to say api.F.feature.v() but only F.feature.v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "instances found:  195\n",
      "avg. # of satellites:  2.24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# collect all clauses that contain the target verb CKB with a phrase function of predicate\n",
    "\n",
    "target = 'CKB['\n",
    "stem = 'qal' # we are only studying the qal stem for now\n",
    "\n",
    "# derived from valency corrections notebook (Roorda&Dyk)\n",
    "# for now we take only functions that have a regular verb \n",
    "predicate_functions = {'Pred', 'PreS', 'PreO', 'PreC', 'PtcO', 'PrcS'} \n",
    "# complements | adjuncts:\n",
    "cmpl_adj = {'Cmpl','Adju','Time','Supp', 'Objc','Subj','ModS','NCoS','IntS','PrAd'} \n",
    "\n",
    "def find_satellites(target, stem, pred_functions, comp_adj):\n",
    "    '''\n",
    "    takes a lemma&stem and returns a dict containing:\n",
    "    results[clause_atom_node] = [phrase_function for pf in relevant_phrase_functions]\n",
    "    '''\n",
    "    satellites = dict()\n",
    "    for word in F.otype.s('word'):\n",
    "        lex = F.lex.v(word)\n",
    "        if lex != target:\n",
    "            continue\n",
    "        phrase_node = L.u(word, otype='phrase')[0]\n",
    "        phrase_func = F.function.v(phrase_node)\n",
    "        if phrase_func not in pred_functions or F.vs.v(word) != stem:\n",
    "            continue\n",
    "        clause_node = L.u(phrase_node, otype = 'clause_atom')[0]\n",
    "        phrase_functs = list(F.function.v(phrase) for phrase in L.d(clause_node, otype = 'phrase')\\\n",
    "                             if F.function.v(phrase) in pred_functions | comp_adj)\n",
    "        satellites[clause_node] = phrase_functs\n",
    "    return satellites\n",
    "\n",
    "ckb_sats = find_satellites(target, stem, predicate_functions, cmpl_adj)\n",
    "\n",
    "# display the average length of the gathered data\n",
    "def avg_data(data_dictionary):\n",
    "    total_datPoints = len(data_dictionary.values())\n",
    "    total_datLengths = sum(len(datpoint) for datpoint in data_dictionary.values())\n",
    "    return round(total_datLengths / total_datPoints, 2)\n",
    "\n",
    "print()\n",
    "print('instances found: ', len(ckb_sats))\n",
    "print('avg. # of satellites: ', avg_data(ckb_sats))        \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inventory of all registered functions for CKB:\n",
      "{'Subj', 'PreC', 'Adju', 'PreO', 'Pred', 'IntS', 'PreS', 'Objc', 'Time', 'Cmpl'}\n"
     ]
    }
   ],
   "source": [
    "print('Inventory of all registered functions for CKB:')\n",
    "print(set(function for function_list in ckb_sats.values() for function in function_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the elements in the function codes above are superfluous or unnecessarily connected. For example: `PreS` with a suffixed subject belongs in the same category as `Pred+Subj`. In the `IntS` function, the interrogative is (at this point) superfluous for phrase-level valency function, but the `S` of subject is necessary. Let's simplify those labels. While we're at it, we'll convert the codes into more readable forms and also convert the `Objc` label into a direct object label.<br><br>\n",
    "We keep the copy of the original dictionary in case we find that the altered labels contain valuable data later during the analysis stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OLD average num. of satellites per instance:\n",
      "2.24 \n",
      "\n",
      "New average num. of satellites per instance:\n",
      "2.41\n"
     ]
    }
   ],
   "source": [
    "func_convert = {'Subj' : 'subject',    # the new labels will be attached to a string and .split()'ed\n",
    "                'PreC' : 'predicate complement',\n",
    "                'Adju' : 'adjunct',\n",
    "                'PreO' : 'predicate direct_object',\n",
    "                'Pred' : 'predicate',\n",
    "                'IntS' : 'subject',\n",
    "                'PreS' : 'predicate subject',\n",
    "                'Objc' : 'direct_object',\n",
    "                'Time' : 'adjunct', # cf. Roorda&Dyk notebook\n",
    "                'Cmpl' : 'complement'\n",
    "                }\n",
    "\n",
    "simple_ckb_sats = dict()\n",
    "\n",
    "for instance, satellites in ckb_sats.items():\n",
    "    satellites = ' '.join(func_convert[fn] for fn in satellites)\n",
    "    simple_ckb_sats[instance] = satellites.split()\n",
    "    \n",
    "print('\\nOLD average num. of satellites per instance:')\n",
    "print(avg_data(ckb_sats),'\\n')\n",
    "print('New average num. of satellites per instance:')\n",
    "print(avg_data(simple_ckb_sats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ a higher avg. means we've succeeded in splitting several of the combined satellites.<br><br>\n",
    "\n",
    "There remains the potential for variations in phrase-order to affect simple comparisons; however, we also need to preserve duplicate functions. In order to accomplish this we affix a '2' to the second label function. Afterwards, we convert the data to a set so it will be suitable for comparison.<br><br>\n",
    "\n",
    "We continue to make modifications only to the `simple_ckb_sats` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for instance, sats in simple_ckb_sats.items():\n",
    "    new_sats = list()\n",
    "    for sat in sats:\n",
    "        sat_count = new_sats.count(sat)\n",
    "        new_sats.append('{}{}'.format(sat, sat_count if sat_count else ''))\n",
    "    simple_ckb_sats[instance] = set(new_sats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to present some data...<br>\n",
    "I'll be using some HTML tricks inspired by [Gino Kalkman's notebook](https://github.com/ETCBC/Biblical_Hebrew_Analysis/blob/master/Miscellaneous/AsyndeticClauseFunctions.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th style=\"text-align: center\">Total</th><th style=\"text-align: center\">Pattern</th><tr><tr><td>50</td><td>complement, predicate</td><tr><td>43</td><td>complement, predicate, subject</td><tr><td>25</td><td>predicate</td><tr><td>13</td><td>predicate, subject</td><tr><td>12</td><td>adjunct, predicate</td><tr><td>9</td><td>complement, complement1, predicate, subject</td><tr><td>9</td><td>complement, complement1, predicate</td><tr><td>7</td><td>adjunct, complement, predicate</td><tr><td>7</td><td>direct_object, predicate</td><tr><td>5</td><td>adjunct, predicate, subject</td><tr><td>4</td><td>adjunct, direct_object, predicate</td><tr><td>3</td><td>adjunct, complement, predicate, subject</td><tr><td>3</td><td>direct_object, predicate, subject</td><tr><td>2</td><td>direct_object, direct_object1, predicate, subject</td><tr><td>1</td><td>adjunct, adjunct1, complement, predicate</td><tr><td>1</td><td>complement, predicate, predicate1</td><tr><td>1</td><td>complement, direct_object, predicate, subject</td></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To-Do: write the HTML generating code into a function so I can re-use it to generate tables.\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "ckb_simple_sTats = col.Counter()\n",
    "\n",
    "for instance, pattern in simple_ckb_sats.items():\n",
    "    ckb_simple_sTats[ tuple(sorted(pattern)) ] += 1\n",
    "\n",
    "# write an HTML table with the data:\n",
    "text_style = ' style=\"text-align: center\"'    # centered style for all text\n",
    "tab_header = '<tr><th{style}>Total</th><th{style}>Pattern</th><tr>'.format(style=text_style) # table header\n",
    "row_templ = '<tr><td{style}>{total}</td><td{style}>{pattern}</td>' # template for each datarow\n",
    "ckb_simple_results = '<table>' + tab_header # This is the complete code we'll add to. Add the header first.\n",
    "# loop through all of the stats and add them to the code above\n",
    "for result, amount in sorted(ckb_simple_sTats.items(), key = lambda k: -k[1]):\n",
    "    ckb_simple_results += row_templ.format(total = amount, \n",
    "                                           pattern = ', '.join(result), \n",
    "                                           style = '')\n",
    "    \n",
    "ckb_simple_results += '</table>' # complete the HTML table tag\n",
    "\n",
    "display(HTML(ckb_simple_results)) # display results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note that the elements in these patterns are sorted alphabetically, not in the order of occurrence."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
