{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `שכב` valency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook last modified on 2017-01-15 17:43:14.195322\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "last_modified = datetime.now()\n",
    "print('Notebook last modified on {}'.format(last_modified.__str__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Methodology\n",
    "The goal is to inventory and categorise the various satellites surrounding the verb שכב (\"to lie\") in biblical Hebrew in order to determine which elements give rise to which semantic meanings of שכב. Valency tracks the interaction between semantics and syntax.<br>\n",
    "<br>\n",
    "In Dyk et al. they suggest that few \"watertight\" methods exist to separate obligatory, complement functions from non-obligatory, adjunctive functions. (see [Dyk, Glanz, Oosting, \"Analysing Valence Patterns,\"](https://shebanq.ancient-data.org/shebanq/static/docs/methods/2014_Dyk_jnsl.pdf) 4-5). They apply a \"distributional method\" as follows:\n",
    "\n",
    "* \"Collect all occurrences of a verb with the complete patterns of elements occurring in the data.\"\n",
    "* \"Sort these by pattern.\"\n",
    "* \"Analyse the differences between the various patterns, observing what relation the separate sentence constituents have to the verb.\" *(Dyk et al., 6)*\n",
    "\n",
    "Which elements to use? Dyk et al. use:\n",
    "* \"predicate (Pred), subject (Subj), object (Objc), complement (Cmpl), adjunct (Adju).\" (7)\n",
    "\n",
    "The [Roorda/Dyk valency corrections notebook](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/corr_enrich.html) informed and inspired many of the queries created in this notebook.<br><br>\n",
    "Let's begin by applying the simplest measures first. We'll keep track of how many examples of the predicate we've accounted for as we work from simpler to more complex patterns. \n",
    "<br><br>\n",
    "**Here's the objectives:**<br>\n",
    "1. Inventory phrase functions for relevant phrase functions; organise by these groups.\n",
    "    * This part establishes the valency type of the verb. Is it transitive or intransitive? Monovalent, divalent, or trivalent? Are there examples of valence expansion or valence reduction?\n",
    "2. Further subdivide the general patterns with semantic/lexical distinctions\n",
    "    * Roorda & Dyk's valency correction notebook (above) further sub-categorises functions based on semantic distinctions such as location, time, instrumentality, and a few more. The database as-is does not contain these distinctions. But some may be inferred from the [features](https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/0_overview.html): \n",
    "        * **`nametype`**\n",
    "        * **`gloss`** (used in conjunction with a resource like [WordNet](http://www.nltk.org/howto/wordnet.html) or [FrameNet](http://www.nltk.org/howto/framenet.html) through the NLTK package)\n",
    "        * **`uvf`** (for ה locative markers).\n",
    "    * Perhaps also different prepositions might give rise to different senses?\n",
    "\n",
    "Procedural question: should the order of elements matter? For the time being, let's keep things simple by ignoring the order of elements. This is something that can be analysed secondarily. Or we can go back if necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.3.0\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "109 features found and 0 ignored\n",
      "\n",
      "  0.00s loading features ...\n",
      "   |     0.04s B otype                from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B book                 from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.00s B chapter              from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B verse                from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.18s B g_cons_utf8          from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.19s B g_word_utf8          from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.18s B lex_utf8             from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.12s B function             from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.13s B pdp                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.15s B sp                   from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.17s B vs                   from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.12s B prs                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.10s B st                   from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.20s B rela                 from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.13s B lex                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.08s B g_prs_utf8           from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.17s B nu                   from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.00s B nametype             from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.20s B ls                   from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B gloss                from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.13s B uvf                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.00s B freq_lex             from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.00s Feature overview: 103 nodes; 5 edges; 1 configs; 7 computeds\n",
      "  6.74s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "from collections import *\n",
    "from pprint import pprint\n",
    "from tf.fabric import Fabric\n",
    "\n",
    "TF = Fabric(modules='Hebrew/etcbc4c')\n",
    "print()\n",
    "api = TF.load(\"\"\"otype\n",
    "                 book chapter verse\n",
    "                 function pdp sp vs prs st rela\n",
    "                 lex g_cons_utf8 g_word_utf8 g_prs_utf8 nu\n",
    "                 lex_utf8\n",
    "                 nametype ls gloss uvf\n",
    "                 freq_lex\n",
    "                \"\"\")\n",
    "\n",
    "api.makeAvailableIn(globals()) # so we don't have to say api.F.feature.v() but only F.feature.v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "instances found:  195\n",
      "avg. # of satellites:  2.21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# collect all clauses that contain the target verb CKB with a phrase function of predicate\n",
    "\n",
    "target = 'CKB['\n",
    "stem = 'qal' # we are only studying the qal stem for now\n",
    "\n",
    "# derived from valency corrections notebook (Roorda&Dyk)\n",
    "# for now we take only functions that have a regular verb \n",
    "predicate_functions = {'Pred', 'PreS', 'PreO', 'PreC', 'PtcO', 'PrcS'} \n",
    "# complements | adjuncts:\n",
    "cmpl_adj = {'Cmpl','Adju','Loca','Supp', 'Objc','Subj','ModS','NCoS','IntS','PrAd'} \n",
    "\n",
    "def find_satellites(target, stem, pred_functions, comp_adj):\n",
    "    '''\n",
    "    takes a lemma&stem and returns a dict containing:\n",
    "    results[clause_atom_node] = [phrase_function for pf in relevant_phrase_functions]\n",
    "    '''\n",
    "    satellites = dict()\n",
    "    for word in F.otype.s('word'):\n",
    "        lex = F.lex.v(word)\n",
    "        if lex != target:\n",
    "            continue\n",
    "        phrase_node = L.u(word, otype='phrase')[0]\n",
    "        phrase_func = F.function.v(phrase_node)\n",
    "        if phrase_func not in pred_functions or F.vs.v(word) != stem:\n",
    "            continue\n",
    "        clause_node = L.u(phrase_node, otype = 'clause_atom')[0]\n",
    "        phrase_nodeS = L.d(clause_node, otype = 'phrase')\n",
    "        phrase_functs = list(F.function.v(phrase) for phrase in phrase_nodeS if F.function.v(phrase)\\\n",
    "                             in pred_functions | comp_adj)\n",
    "        satellites[clause_node] = phrase_functs\n",
    "    return satellites\n",
    "\n",
    "ckb_sats = find_satellites(target, stem, predicate_functions, cmpl_adj)\n",
    "\n",
    "# display the average length of the gathered data\n",
    "def avg_data(data_dictionary):\n",
    "    total_datPoints = len(data_dictionary.values())\n",
    "    total_datLengths = sum(len(datpoint) for datpoint in data_dictionary.values())\n",
    "    return round(total_datLengths / total_datPoints, 2)\n",
    "\n",
    "print()\n",
    "print('instances found: ', len(ckb_sats))\n",
    "print('avg. # of satellites: ', avg_data(ckb_sats))        \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inventory of all registered functions for CKB:\n",
      "{'Cmpl', 'Loca', 'Objc', 'Pred', 'PreC', 'PreS', 'Adju', 'PreO', 'Subj', 'IntS'}\n"
     ]
    }
   ],
   "source": [
    "print('Inventory of all registered functions for CKB:')\n",
    "print(set(function for function_list in ckb_sats.values() for function in function_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the elements in the function codes above are superfluous or unnecessarily connected. For example: `PreS` with a suffixed subject belongs in the same category as `Pred+Subj`. In the `IntS` function, the interrogative is (at this point) superfluous for phrase-level valency function, but the `S` of subject is necessary. Let's simplify those labels. While we're at it, we'll convert the codes into more readable forms and also convert the `Objc` label into a direct object label.<br><br>\n",
    "We keep the copy of the original dictionary in case we find that the altered labels contain valuable data later during the analysis stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OLD average num. of satellites per instance:\n",
      "2.21 \n",
      "\n",
      "New average num. of satellites per instance:\n",
      "2.38\n"
     ]
    }
   ],
   "source": [
    "# the new labels will be attached to a string and .split()'ed:\n",
    "func_convert = {'Subj' : 'subject',    \n",
    "                'PreC' : 'predicate complement',\n",
    "                'Adju' : 'adjunct',\n",
    "                'PreO' : 'predicate direct_object',\n",
    "                'Pred' : 'predicate',\n",
    "                'IntS' : 'subject',\n",
    "                'PreS' : 'predicate subject',\n",
    "                'Objc' : 'direct_object',\n",
    "                'Cmpl' : 'complement',\n",
    "                'Loca' : 'complement'\n",
    "                }\n",
    "\n",
    "simple_ckb_sats = dict()\n",
    "\n",
    "for instance, satellites in ckb_sats.items():\n",
    "    satellites = ' '.join(func_convert[fn] for fn in satellites)\n",
    "    simple_ckb_sats[instance] = satellites.split()\n",
    "    \n",
    "print('\\nOLD average num. of satellites per instance:')\n",
    "print(avg_data(ckb_sats),'\\n')\n",
    "print('New average num. of satellites per instance:')\n",
    "print(avg_data(simple_ckb_sats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ a higher avg. means we've succeeded in splitting several of the combined satellites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to present some data...<br>\n",
    "I'll be using some HTML tricks inspired by [Gino Kalkman's notebook](https://github.com/ETCBC/Biblical_Hebrew_Analysis/blob/master/Miscellaneous/AsyndeticClauseFunctions.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's write some functions for displaying some statistics:\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def generate_table(fieldnames, data, style=''):\n",
    "    '''\n",
    "    returns HTML table when fed a fieldnames list and list of tuples in order\n",
    "    optional argument to configure text style\n",
    "    '''\n",
    "    table_code = '<table>'     # here is where all the code will be assembled\n",
    "    table_row = '<tr>{}</tr>'  # template for table rows\n",
    "    # assemble table_header\n",
    "    table_header = ''\n",
    "    for fieldname in fieldnames:\n",
    "        table_header += '<th{style}>{header}</th>'.format(style=style, \n",
    "                                                          header=fieldname)\n",
    "    # assemble table_rows\n",
    "    table_rows = ''\n",
    "    for data_tuple in data:\n",
    "        row = ''\n",
    "        for data in data_tuple:    \n",
    "            row += '<td{style}>{data}</td>'.format(style='',\n",
    "                                                   data=data)\n",
    "        table_rows += table_row.format(row)\n",
    "    # complete the code:\n",
    "    table_code += table_row.format(table_header)\n",
    "    table_code += table_row.format(table_rows)\n",
    "    table_code += '</table>'\n",
    "    # display the code\n",
    "    display(HTML(table_code))\n",
    "    \n",
    "def percent(amount, total):\n",
    "    '''\n",
    "    return a simple percentage\n",
    "    '''\n",
    "    return round((amount/total)*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th style=\"text-align:center\">Total</th><th style=\"text-align:center\"> % </th><th style=\"text-align:center\">Pattern</th></tr><tr><tr><td>54</td><td>27.69</td><td>('complement', 'predicate')</td></tr><tr><td>42</td><td>21.54</td><td>('complement', 'predicate', 'subject')</td></tr><tr><td>26</td><td>13.33</td><td>('predicate',)</td></tr><tr><td>16</td><td>8.21</td><td>('predicate', 'subject')</td></tr><tr><td>11</td><td>5.64</td><td>('complement', 'complement', 'predicate', 'subject')</td></tr><tr><td>11</td><td>5.64</td><td>('complement', 'complement', 'predicate')</td></tr><tr><td>8</td><td>4.1</td><td>('direct_object', 'predicate')</td></tr><tr><td>7</td><td>3.59</td><td>('adjunct', 'predicate')</td></tr><tr><td>6</td><td>3.08</td><td>('adjunct', 'complement', 'predicate')</td></tr><tr><td>3</td><td>1.54</td><td>('direct_object', 'predicate', 'subject')</td></tr><tr><td>3</td><td>1.54</td><td>('adjunct', 'direct_object', 'predicate')</td></tr><tr><td>2</td><td>1.03</td><td>('adjunct', 'complement', 'predicate', 'subject')</td></tr><tr><td>2</td><td>1.03</td><td>('direct_object', 'direct_object', 'predicate', 'subject')</td></tr><tr><td>2</td><td>1.03</td><td>('adjunct', 'predicate', 'subject')</td></tr><tr><td>1</td><td>0.51</td><td>('complement', 'direct_object', 'predicate', 'subject')</td></tr><tr><td>1</td><td>0.51</td><td>('complement', 'predicate', 'predicate')</td></tr></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# assemble some stats to display:\n",
    "ckb_simple_sTats = Counter()\n",
    "for instance, pattern in simple_ckb_sats.items():\n",
    "    ckb_simple_sTats[ tuple(sorted(pattern)) ] += 1\n",
    "    \n",
    "# -- data for the HTML viewer -- \n",
    "\n",
    "fieldnames = ['Total',' % ', 'Pattern']\n",
    "all_patts = sum(ckb_simple_sTats.values())\n",
    "# a list of dicts with fieldname keys:\n",
    "ckb_simple_data = list( (total, percent(total, all_patts), pattern)\\\n",
    "                       for pattern, total in sorted(ckb_simple_sTats.items(), key = lambda k: -k[1])\n",
    "                      ) \n",
    "# display table:\n",
    "print()\n",
    "generate_table(fieldnames, ckb_simple_data, style=' style=\"text-align:center\"')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note that the elements in these patterns are sorted alphabetically, not in the order of occurrence.<br><br>\n",
    "\n",
    "Some first observations:<br>\n",
    "* ~ **`53.84%`** of the patterns have only\n",
    "    * 1 complement or\n",
    "    * 1 adjunct\n",
    "    \n",
    "    \n",
    "* ~ **`19.49%`** with\n",
    "    * 0 other elements except for a subject\n",
    "    \n",
    "    \n",
    "* ~ **`5.13%`** with\n",
    "    * 1 direct object\n",
    "    \n",
    "The rest of the patterns have complex mixtures of complements, adjuncts, and objects as well as doubles.<br><br>\n",
    "\n",
    "Perhaps there are some further refinements we can apply to the categories. It's not certain whether the presence of a subject is relevant for the phrase functions. But it's impossible to know until the examples can be manually sorted. Before we move to that stage, we'll add some more information about the lexical and semantic qualities (and subcategories) of the groups observed above.\n",
    "\n",
    "## Step 2: Lexical and Semantic Categories\n",
    "\n",
    "We now have some basic groups and information to build further queries upon. This next step entails measuring the lexical and semantic qualities of the שכב satellites. Some features we're looking for:\n",
    "* locative lexemes - lexemes that imply spatial distinctions, and thus movement\n",
    "* agentive lexemes - lexemes that imply reception of the action\n",
    "* instrumental lexemes - lexemes that imply the use of objects/tools in the action\n",
    "\n",
    "Further subclassifications may break down by preposition use. See, for example, the activity in the Roorda&Dyk corrections notebook which contain ל and כ objects. That notebook already contains some rules defined for L/K objects:\n",
    "* \"start with either preposition L or K and\n",
    "* the L or K in question does not carry a pronominal suffix\n",
    "* should also not be followed by a body part\" \n",
    "([Roorda&Dyk](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/corr_enrich.html), \"Complements as LK Objects\")\n",
    "\n",
    "Since these kinds of features are not presently stored in ETCBC4c, we have to use a few tricks to procure them. In some cases, items considered \"adjuncts\" (i.e., \"unnecessary\"), will need to be reconsidered as complements (key to the semantic meaning). **Locative** and **agentive** lexemes will be facilitated by the features:\n",
    "* `nametype`\n",
    "    * = `topo` (place)\n",
    "    * = `pers` (person)\n",
    "    * = `gens` (people)\n",
    "* `uvf`\n",
    "    * contains locative ה\n",
    "* `ls`\n",
    "    * = `gentilic` (i.e. demonym)\n",
    "    \n",
    "I also would like to experiment with deploying Framenet or Wordnet combined with the `gloss` feature as a method of categorising lexemes. This may prove especially useful for **instrumental** terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_1 -  106 members\n",
      "('complement', 'predicate', 'predicate')\n",
      "('complement', 'predicate')\n",
      "('adjunct', 'predicate')\n",
      "('complement', 'predicate', 'subject')\n",
      "('adjunct', 'predicate', 'subject')\n",
      "\n",
      "group_2 -  11 members\n",
      "('direct_object', 'predicate', 'subject')\n",
      "('direct_object', 'predicate')\n",
      "\n",
      "group_3 -  42 members\n",
      "('predicate', 'subject')\n",
      "('predicate',)\n",
      "\n",
      "group_4 -  36 members\n",
      "('adjunct', 'complement', 'predicate', 'subject')\n",
      "('complement', 'complement', 'predicate')\n",
      "('adjunct', 'complement', 'predicate')\n",
      "('complement', 'direct_object', 'predicate', 'subject')\n",
      "('adjunct', 'direct_object', 'predicate')\n",
      "('direct_object', 'direct_object', 'predicate', 'subject')\n",
      "('complement', 'complement', 'predicate', 'subject')\n",
      "\n",
      "Group_all 195\n"
     ]
    }
   ],
   "source": [
    "# First we need to organise the 4 simple groups we observed above.\n",
    "# We treat group_4, mixed satellites, as a catch-all for now,\n",
    "#   so that we can deal with the simpler structures first.\n",
    "\n",
    "basic_groups = defaultdict(set)       # a dict keyed by group, valued by sets of clause nodes\n",
    "basic_groups_check = defaultdict(set) # to double-check our results\n",
    "\n",
    "# rules for groups based on counts:\n",
    "         # (adj, compl, d.o.)\n",
    "group_rules = { (1,0,0) : 'group_1', # 1 adj ø else\n",
    "                (0,1,0) : 'group_1', # 1 comp ø else\n",
    "                (0,0,1) : 'group_2', # 1 d.o. ø else\n",
    "                (0,0,0) : 'group_3', # ø else\n",
    "               #all else : group_4\n",
    "              }\n",
    "\n",
    "for clause_node, pattern in simple_ckb_sats.items():\n",
    "    adj_count = pattern.count('adjunct')\n",
    "    comp_count = pattern.count('complement')\n",
    "    do_count = pattern.count('direct_object')\n",
    "    count = (adj_count,comp_count,do_count) \n",
    "    if count in group_rules:\n",
    "        group = group_rules[count]\n",
    "    else:\n",
    "        group = 'group_4'\n",
    "    basic_groups[group].add(clause_node)\n",
    "    basic_groups_check[group].add(tuple(sorted(pattern)))\n",
    "        \n",
    "# -- Double Check Our Results -- #\n",
    "        \n",
    "total_check = 0\n",
    "for group, group_nodes in sorted(basic_groups.items()):\n",
    "    total_check += len(group_nodes)\n",
    "    print(group+' - ', len(group_nodes), 'members')\n",
    "    for patt in basic_groups_check[group]:\n",
    "        print(patt)\n",
    "    print()\n",
    "print('Group_all', total_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good. We can move on to measuring semantics throughout each group to create more nuanced categories.\n",
    "\n",
    "The key to these queries is the **noun**, upon which we'll perform checks for semantic data. The Roorda&Dyk notebook contains a nice example of measuring semantic ideas with a scoring system. I'll keep this in mind as I move ahead...\n",
    "\n",
    "First, we work with group 1 and inventory the patterns in the complement phrases. The inventory will provide a basis for the semantic work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Total</th><th>Pattern</th></tr><tr><tr><td>58</td><td>('prep', 'subs')</td></tr><tr><td>21</td><td>('prep',)</td></tr><tr><td>5</td><td>('subs',)</td></tr><tr><td>3</td><td>('advb',)</td></tr><tr><td>3</td><td>('prep', 'art', 'subs')</td></tr><tr><td>2</td><td>('verb',)</td></tr><tr><td>2</td><td>('prep', 'adjv', 'subs')</td></tr><tr><td>2</td><td>('prep', 'subs', 'prep', 'subs', 'subs')</td></tr><tr><td>2</td><td>('prep', 'subs', 'art', 'adjv')</td></tr><tr><td>1</td><td>('prep', 'nmpr', 'subs', 'subs')</td></tr><tr><td>1</td><td>('prep', 'subs', 'subs')</td></tr><tr><td>1</td><td>('prep', 'subs', 'art', 'subs')</td></tr><tr><td>1</td><td>('prep', 'subs', 'nmpr')</td></tr><tr><td>1</td><td>('prep', 'subs', 'conj', 'prep', 'subs')</td></tr><tr><td>1</td><td>('prep', 'subs', 'prep', 'subs', 'nmpr')</td></tr><tr><td>1</td><td>('prep', 'prin')</td></tr><tr><td>1</td><td>('prep', 'art', 'subs', 'art', 'prde')</td></tr></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GROUP 1 - SINGLE ADJUNCT OR COMPLEMENTS\n",
    "# WORD FUNCTION INVENTORIES\n",
    "# query and inventory word-level, internal phrase functions in group 1\n",
    "\n",
    "basic_groups['group_1']\n",
    "\n",
    "def getPhraseInventory(group, targetElements, ):\n",
    "    group_phrases = dict()\n",
    "    group_stats = Counter()\n",
    "    for clause_atom in group:\n",
    "        phrases = L.d(clause_atom, otype='phrase')\n",
    "        target_phrase = None\n",
    "        for phrase in phrases:\n",
    "            func = F.function.v(phrase)\n",
    "            simple_form = func_convert[func].split() if func in func_convert else func\n",
    "            target_phrase = phrase if targetElements & set(simple_form)\\\n",
    "                                   else target_phrase\n",
    "        \n",
    "        pattern = tuple(F.pdp.v(word) for word in L.d(target_phrase, otype='word'))\n",
    "        \n",
    "        # nouns (and adverbs/adjectives) that do not function as a subject\n",
    "        objNouns = tuple(w for w in L.d(target_phrase, otype='word') \n",
    "                           if all([\n",
    "                                   F.pdp.v(w) in {'subs','prps','prin','adjv','advb'}\n",
    "                                   or F.prs.v(w) not in {'n/a','absent'},\n",
    "                                   not {'rec','atr'} & set(F.rela.v(subPhrase) for subPhrase in L.u(w, otype='subphrase')),\n",
    "                                   F.function.v((L.u(w,otype='phrase'))) != 'Subj'\n",
    "                                   ])\n",
    "                        ) #/tuple\n",
    "\n",
    "        \n",
    "        group_phrases[clause_atom] = {'phrase':target_phrase, 'objNouns':objNouns, 'pattern':pattern}\n",
    "        group_stats[pattern] += 1\n",
    "        \n",
    "    return group_phrases, group_stats\n",
    "\n",
    "group1 = getPhraseInventory(basic_groups['group_1'], {'complement','adjunct'})\n",
    "\n",
    "group1_phrases = group1[0]\n",
    "    \n",
    "group1_header = ['Total','Pattern']\n",
    "display_group1 = list((a,p) for p, a in sorted(group1[1].items(), key=lambda k: -k[1]))\n",
    "\n",
    "generate_table(group1_header, display_group1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show the prominent position of the preposition in group 1 constructions.\n",
    "\n",
    "57 of the results contain only a preposition and a substantive. We'll focus first on this simpler form. Hopefully this will give a basis on which to process the rarer, more complex examples.\n",
    "\n",
    "### Sense Generator\n",
    "We use a sense generator that will return 1 of 3 categories for a given lemma. The categories are:\n",
    "* person\n",
    "* place\n",
    "* thing/object\n",
    "\n",
    "The \"machine\" will use 4 sources to make its decisions: \n",
    "1. existing features in the ETCBC\n",
    "2. lists of categorised lexemes from the [Roorda/Dyk notebook](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/corr_enrich.html)\n",
    "3. sources 3 and 4 are special projects created for these kinds of queries:\n",
    "    * see [valency_wordlists](https://github.com/codykingham/textfabric_notebooks/blob/master/valency_wordlists)\n",
    "    * (3) generated category lists from [openscriptures' BDB lexicon](https://github.com/openscriptures/HebrewLexicon) using part-of-speech tags in BDB \n",
    "    * (4) generated category lists from Wordnet, using hypernym relations\n",
    "\n",
    "Each of the sources will count as 1 or more parameters, that, if fulfilled, will go towards a score for the given lexeme object. The categorisation is based on a simple majority, but scores can also be returned with a \"strength\", for example, 3/3 or 2/3, depending on how many parameters are met out of how many are applicable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../valency_wordlists/bdbCategories.json') as bdbFile:\n",
    "    bdbCategories = json.load(bdbFile)\n",
    "    \n",
    "with open('../valency_wordlists/wordnetCategories.json') as wnFile:\n",
    "    wnCategories = json.load(wnFile)\n",
    "\n",
    "with open('../valency_wordlists/RoordaDykCategories.json') as rdFile:\n",
    "    rdCategories = json.load(rdFile)\n",
    "    \n",
    "def testLocation(word_node, info=False):\n",
    "    lexeme = F.lex.v(word_node)\n",
    "    # SOURCE 1, ETCBC4c\n",
    "    lex_obj = L.u(word_node, otype = 'lex')[0]\n",
    "    univalent_final = 1 if F.uvf.v(word_node) == 'H' else 0\n",
    "    name_type = 1 if F.nametype.v(lex_obj) == 'topo' else 0\n",
    "    s1_score = sum((univalent_final, name_type))\n",
    "    # SOURCE 2, ROORDA/DYK\n",
    "    s2_score = 1 if lexeme in rdCategories and rdCategories[lexeme]['category'] == 'place' else 0\n",
    "    # SOURCE 3, BDB categories list\n",
    "    s3_score = 1 if lexeme in bdbCategories and bdbCategories[lexeme]['category'] == 'place' else 0\n",
    "    # source 4, wordnet categories list\n",
    "    s4_score = 1 if lexeme in wnCategories and wnCategories[lexeme]['cat'] == 'place' else 0\n",
    "    if not info: return sum((s1_score, s2_score, s3_score, s4_score))\n",
    "    else: return { 's1':s1_score, 's2':s2_score,'s3':s3_score, 's4':s4_score}\n",
    "    \n",
    "def testAgency(wordNode, info=False):\n",
    "    # SOURCE 1, ETCBC4c\n",
    "    lexN = L.u(wordNode, otype = 'lex')[0]\n",
    "    lex = F.lex.v(lexN)\n",
    "    nametype = 1 if F.nametype.v(lexN) == 'pers' else 0\n",
    "    gentilic = 1 if F.ls.v(wordNode) == 'gntl' else 0\n",
    "    pronoun = 1 if F.sp.v(wordNode) == 'prps' or F.pdp.v(wordNode) == 'prps' else 0\n",
    "    s1Score = sum((nametype,gentilic,pronoun))\n",
    "    # SOURCE 2, Roorda/Dyk\n",
    "    s2Score = 1 if lex in rdCategories and rdCategories[lex]['category'] == 'agent' else 0\n",
    "    # Source 3 - BDB categories\n",
    "    s3Score = 1 if lex in bdbCategories and bdbCategories[lex]['category'] == 'agent' else 0\n",
    "    # Source 4 - WN categories\n",
    "    s4Score = 1 if lex in wnCategories and wnCategories[lex]['cat'] == 'agent' else 0\n",
    "    if not info: return sum((s1Score, s2Score, s3Score, s4Score))\n",
    "    else: return { 's1':s1Score, 's2':s2Score,'s3':s3Score, 's4':s4Score}\n",
    "    \n",
    "def abstractObject(wordNode, info=False):\n",
    "    lex = F.lex.v(wordNode)\n",
    "    # Source 1, BDB categories\n",
    "    s1score = 1 if lex in bdbCategories and bdbCategories[lex]['subcategory'] == 'abstract' else 0\n",
    "    s2score = 1 if lex in wnCategories and wnCategories[lex]['subcategory'] == 'abstract' else 0\n",
    "    if not info: return(sum((s1score, s2score)))\n",
    "    else: return {'BDB': s1score, 'WordNet':s2score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export spreadsheet for processing these basic patterns\n",
    "\n",
    "The spreadsheet contains sections that are pre-organised based on the estimations above. They are first organised by the simplest parameters to the more obscure parameters:\n",
    "\n",
    "1. for every phrase in group 1, gather the number of prepositions, preposition lexeme, number of substantives, and category of substantives (agency, location, or ?). \n",
    "\n",
    "2. sort the groups into sub-groups by the following categories:\n",
    "    * sort by number of prepositions from least to greatest\n",
    "        * these are then sorted by the number of substantives\n",
    "            * these are sub-sorted again by the lexeme of the preposition\n",
    "                * these are once again subsorted by the category of lexeme\n",
    "                \n",
    "The resulting spreadsheet is cleanly categorised in ascending order from the simplest structures to the most complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## to fix:\n",
    "\n",
    "* √ include the `Loca` tag! \n",
    "* √ exclude the `Time` tag\n",
    "* √ `PreC` phrases are omitting nouns and adjectives resulting in false positives for the 0,0 category.\n",
    "    * PreC followed by an adverbial expression that communicates location should be included with the 1,0 group.\n",
    "* See clauseN `547260`, 2 Samuel 13.8\n",
    "    * substantives which function as subjects should NOT be included in the noun inventory\n",
    "    * we are looking for nouns that are acted upon or with\n",
    "* include suffixes as noun phrases\n",
    "    * 1,0 is essentially illegitimate\n",
    "    * find: `prep + suffix`\n",
    "* prepositions within time phrases should not be counted\n",
    "    * see Eccl 2.23\n",
    "* words in construct should only be counted once\n",
    "    * see Ruth 3.7\n",
    "    * ft. those words that share the same phrase should be excluded\n",
    "    * see also `st` (state) feature; select 'absolute'\n",
    "    * check for waw conjunction\n",
    "* words that also count as the verb should not be counted as a substantive\n",
    "    * see Prov 23.34; how to deal with this tricky example?\n",
    "    * should the phrase goverened by שכב be reinterpreted as a clause atom?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def testCat(noun):\n",
    "    '''\n",
    "    test whether a given noun registers\n",
    "    for agency or location lexeme types\n",
    "    '''\n",
    "    agency = testAgency(noun)\n",
    "    loca = testLocation(noun)\n",
    "    if agency or loca:\n",
    "        return 'agent' if agency > loca else 'locale'\n",
    "    else:\n",
    "        return '?'\n",
    "\n",
    "def writeGroups(phrasedict):\n",
    "    '''\n",
    "    assemble the groups in order of \n",
    "     1) number of prepositions (least to greatest)\n",
    "     2) number of substantives (least to greatest)\n",
    "    build the row data for the CSV analysis file\n",
    "    '''\n",
    "    subgroups = defaultdict(lambda: defaultdict(list))\n",
    "    for clause, phraseDat in phrasedict.items():\n",
    "        \n",
    "        phrase = phraseDat['phrase']\n",
    "        pattern = phraseDat['pattern']\n",
    "        prepCount = phraseDat['pattern'].count('prep')\n",
    "        nounCount = len(phraseDat['objNouns'])\n",
    "        verb = tuple(F.g_word_utf8.v(v) for v in L.d(clause,otype='word') if F.pdp.v(v) == 'verb')\n",
    "        prepWords = tuple(w for w in L.d(phrase,otype='word') if F.pdp.v(w) == 'prep')\n",
    "        nounWords = phraseDat['objNouns']\n",
    "        \n",
    "        lexWords = tuple(L.u(w,otype='lex')[0] for w in L.d(clause,otype='word'))\n",
    "        infrequents = tuple( str((F.lex.v(l),F.gloss.v(l))) for l in lexWords if F.freq_lex.v(l) < 30)\n",
    "        verse = L.u(phrase, otype='verse')[0]\n",
    "        \n",
    "        subgroups[prepCount][nounCount].append(\n",
    "                                            {\n",
    "                                                'prepCt':prepCount,\n",
    "                                                'nounCt':nounCount,\n",
    "                                                'ref' : T.sectionFromNode(phrase),\n",
    "                                                'clauseNode':clause,\n",
    "                                                'phraseNode': phrase,\n",
    "                                                'verse': T.text(L.d(verse, otype='word')),\n",
    "                                                'phraseType':F.function.v(phrase),\n",
    "                                                'clause': T.text(L.d(clause,otype='word')),\n",
    "                                                'phrase': T.text(L.d(phrase,otype='word')),\n",
    "                                                'rareTerms': '\\n'.join(infrequents), \n",
    "                                                'verb' : ' '.join(verb),\n",
    "                                                'prepositions': ' '.join(F.lex_utf8.v(p) for p in prepWords), \n",
    "                                                'nouns': ' '.join(F.g_word_utf8.v(n) for n in nounWords),\n",
    "                                                'nounTypes':' '.join(testCat(n) for n in nounWords),\n",
    "                                                'notes':' ',\n",
    "                                            })\n",
    "    return subgroups\n",
    "\n",
    "\n",
    "def countParameter(listOfClauseDicts, ParameterKey):\n",
    "    '''\n",
    "    counts the preposition or nountypes amongst all of the target phrases\n",
    "    the return values are used to order the CSV data\n",
    "    '''\n",
    "    inventory = Counter()\n",
    "    for data in listOfClauseDicts:\n",
    "        inventory[data[ParameterKey]] += 1\n",
    "    return inventory\n",
    "    \n",
    "    \n",
    "group1Data = writeGroups(group1_phrases)\n",
    "\n",
    "fieldnames = ['ref','phraseNode','clauseNode','verse','rareTerms','phraseType','clause',\n",
    "              'verb','nouns','phrase','prepCt','nounCt','prepositions','nounTypes','notes']\n",
    "\n",
    "def writeAnalysis(group, filename, fieldnames):\n",
    "    '''\n",
    "    write the pre-assembled groups into the csv analysis file\n",
    "    '''\n",
    "    with open(filename,'w') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames = fieldnames)\n",
    "        writer.writeheader()\n",
    "        for prepCount, nounCounts in sorted(group.items()): # first, sort by number of prepositions\n",
    "            for nounCount, clauseDat in sorted(nounCounts.items()): # second, subsort by number of substantives\n",
    "                wordCatCount= countParameter(clauseDat,'nounTypes') # third, subsort by frequency of a preposition lexeme\n",
    "                Catsort = sorted([(wordCatCount[clause['nounTypes']], clause) for clause in clauseDat], key = lambda k: -k[0])\n",
    "                prepLexCount = countParameter(clauseDat,'prepositions') # fourth, subsort by frequency of word category\n",
    "                prepsort = sorted([(prepLexCount[data[1]['prepositions']],data) for data in Catsort], key = lambda k: -k[0])\n",
    "                finalData = tuple(data[1][1] for data in prepsort)   \n",
    "                writer.writerows(finalData)\n",
    "                \n",
    "writeAnalysis(group1Data,'group1.csv', fieldnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remaking the Groups\n",
    "\n",
    "In light of holes and findings in group 1, it is best to re-create the basic groups into finer distinctions. We can use sub-categories found in group 1 to also further categorise the groups.\n",
    "\n",
    "The groups will be created based on the following set of rules:\n",
    "\n",
    "* preposition\n",
    "    * עם / את\n",
    "        * object == pl אב ('fathers') ?\n",
    "            * **death, figurative idiom**\n",
    "        * object == suffix ?\n",
    "            * **sexual intercourse**\n",
    "        * object == agentive noun ?\n",
    "            * locative satellite in between ? \n",
    "                * **literal action, with/among entity** \n",
    "            * else: \n",
    "                * **sexual intercourse**\n",
    "        * else\n",
    "            * **literal action, with/among entity**\n",
    "\n",
    "    * על / ל / ב / אצל / בין, et al.\n",
    "        * object == abstract object ? \n",
    "            * **literal action, manner of lying**\n",
    "        * object == locative or unknown ?\n",
    "            * ** literal action, spatial direction of act**\n",
    "\n",
    "* ø preposition\n",
    "    * noun/adj/adv == locative ?\n",
    "        * 'sex' category in clause atom? \n",
    "            * **sexual intercourse + adjunctive object**\n",
    "        * else:\n",
    "            * **literal action, spatial direction of act**\n",
    "    * ø noun/adj/adv\n",
    "        * +verbal suffix ?\n",
    "            * **sexual intercourse** \n",
    "        * else\n",
    "            * **literal action, connotates sleep**\n",
    "        \n",
    "        \n",
    "These rules are applied on a per-phrase basis to create phrase categories. Categories can be compounded together to create compound categories. I expect these distinctions to break down once multiple complement/object/adjunct phrases are combined. Once that happens, we'll adjust the rules accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def isObjectNoun(w):\n",
    "    if all([F.pdp.v(w) in {'subs','prps','prin','adjv','advb','nmpr'},\n",
    "            not {'rec','atr'} & set(F.rela.v(subPhrase) for subPhrase in L.u(w, otype='subphrase')),\n",
    "            F.function.v((L.u(w,otype='phrase'))) != 'Subj',\n",
    "            F.rela.v(L.u(w, otype = 'phrase_atom')[0]) not in {'Appo'},\n",
    "           ]):\n",
    "        return True\n",
    "\n",
    "def analyseSatellites(clauseAtom):\n",
    "    satelliteFunctions = {'Adju','PreO','Objc','Cmpl','Loca'}\n",
    "    satellites = (ph for ph in L.d(clauseAtom, otype='phrase') if F.function.v(ph) in satelliteFunctions)\n",
    "    satData = defaultdict(dict)\n",
    "    \n",
    "    for sat in satellites:\n",
    "        words = L.d(sat, otype='word')\n",
    "        prepositions = tuple(w for w in words if F.pdp.v(w) == 'prep')\n",
    "        objectNouns = tuple(w for w in words if isObjectNoun(w))\n",
    "        categories = []\n",
    "        prepAndObjs = []\n",
    "        \n",
    "        # apply the rules following the logic in the above notes\n",
    "        if prepositions:\n",
    "                # get suffixed objects from suffixed prepositions or keep preposition objects\n",
    "            objectNouns = tuple(F.g_prs_utf8.v(prep) for prep in prepositions) \\\n",
    "                          if not objectNouns else objectNouns\n",
    "            prepAndObj = tuple(zip(prepositions, objectNouns)) # map prepositions to their objs\n",
    "            \n",
    "            # Calculate the categories for prepositions:    \n",
    "            for prep, pObj in prepAndObj:\n",
    "                if F.lex.v(prep) in {'<M','>T','>T=='}:\n",
    "                    if F.lex.v(pObj) == '>B/' and F.nu.v(pObj) == 'pl': \n",
    "                        category = 'death'\n",
    "                    elif type(pObj) == str or testAgency(pObj): # suffix obj or agentive obj.\n",
    "                        \n",
    "                        # determine whether an intervening preposional phrase occurs\n",
    "                        # such a phrase can block the sexual meaning as seen in texts like:\n",
    "                        # Job 20.11 and 2 Sam 11.3\n",
    "                        verb = next(w for w in L.d(clauseAtom, otype='word') if F.pdp.v(w) == 'verb')\n",
    "                        first, second = sorted((verb,prep)) # verb can come before or after\n",
    "                        # now calculate all the intervening words and include if one is a preposition\n",
    "                        # but it cannot be a 'with' preposition\n",
    "                        interveningToVerb = set(F.pdp.v(n) for n in range(first+1, second)\n",
    "                                               if F.lex.v(n) not in {'<M','>T','>T=='})\n",
    "                        if any(['prep' in interveningToVerb,\n",
    "                                'literal.prepSpatial' in {cat for sat in satData[clauseAtom]\n",
    "                                                          for cat in satData[clauseAtom][sat]['categories']}\n",
    "                               ]):\n",
    "                            category = 'literal.withEntity'\n",
    "                        else:\n",
    "                            category = 'sex'\n",
    "                    else:\n",
    "                        category = 'literal.withEntity'\n",
    "                else: #'<L','B','L','>YL/','BJN/', et. al\n",
    "                    if abstractObject(pObj) or F.lex.v(prep) == 'K':\n",
    "                        category = 'literal.manner'\n",
    "                    else:\n",
    "                        category = 'literal.prepSpatial'\n",
    "                categories.append(category)\n",
    "                prepAndObjs.extend(prepAndObj)\n",
    "        elif objectNouns:\n",
    "            for obj in objectNouns:\n",
    "                if testLocation(obj):\n",
    "                    category = 'literal.spatialObj'\n",
    "                else:\n",
    "                    if any(['sex' in categories,\n",
    "                            'sex' in {cat for sat in satData[clauseAtom]\n",
    "                                      for cat in satData[clauseAtom][sat]['categories']}]):\n",
    "                        category = 'sex.object'\n",
    "                    else:\n",
    "                        category = 'literal.unknown'\n",
    "                categories.append(category)\n",
    "        else:\n",
    "            verb = tuple(w for w in words if F.function.v(L.u(w, otype='phrase')[0]) == 'PreO')\n",
    "            if verb and F.prs.v(verb[0]):\n",
    "                category = 'sex'\n",
    "            else:\n",
    "                category = 'literal.general'\n",
    "            categories.append(category)\n",
    "        \n",
    "        satData[clauseAtom][sat] = {'prepObjcs': prepAndObjs,\n",
    "                                    'function' : F.function.v(sat),\n",
    "                                    'categories' : categories,\n",
    "                                    'objects' : objectNouns\n",
    "                                   }\n",
    "    if not satData:\n",
    "        satData[clauseAtom] = {}\n",
    "    return satData        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Deuteronomy', 31, 16)\n",
      "הִנְּךָ֥ שֹׁכֵ֖ב עִם־אֲבֹתֶ֑יךָ \n",
      "defaultdict(<class 'dict'>,\n",
      "            {536065: {668165: {'categories': ['death'],\n",
      "                               'function': 'Cmpl',\n",
      "                               'objects': (111042,),\n",
      "                               'prepObjcs': [(111041, 111042)]}}})\n",
      "\n",
      "('2_Samuel', 11, 9)\n",
      "וַיִּשְׁכַּ֣ב אוּרִיָּ֗ה פֶּ֚תַח בֵּ֣ית הַמֶּ֔לֶךְ אֵ֖ת כָּל־עַבְדֵ֣י אֲדֹנָ֑יו \n",
      "defaultdict(<class 'dict'>,\n",
      "            {546951: {700620: {'categories': ['literal.unknown'],\n",
      "                               'function': 'Cmpl',\n",
      "                               'objects': (165892,),\n",
      "                               'prepObjcs': []},\n",
      "                      700621: {'categories': ['literal.withEntity'],\n",
      "                               'function': 'Adju',\n",
      "                               'objects': (165897,),\n",
      "                               'prepObjcs': [(165896, 165897)]}}})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out some samples:\n",
    "\n",
    "for ca in list(basic_groups['group_4'])[:2]:\n",
    "    print(T.sectionFromNode(ca))\n",
    "    print(T.text(L.d(ca, otype='word')))\n",
    "    pprint(analyseSatellites(ca))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the New Groups to CSV\n",
    "\n",
    "1. apply the new function to all clauses in the ckb_sats dictionary\n",
    "2. organise the groups based on 3 tiers of subcategorisation: \n",
    "   * A. the most prevalent satellite counts within all clauses\n",
    "   * B. the most prevalent satellite categories and category combinations within A\n",
    "   * C. the most prevalent prepositional phrases within B\n",
    "3. gather reference information, plain text, labels, etc. for the csv doc\n",
    "4. write to the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Analyse satellites\n",
    "ckbSatellites = {}\n",
    "for clauseAtom in ckb_sats:\n",
    "    ckbSatellites.update(analyseSatellites(clauseAtom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2. arrange the group orderings\n",
    "# A.\n",
    "satCounts = defaultdict(list)\n",
    "for clauseAtom, satellites in ckbSatellites.items():\n",
    "    satCount = len(satellites)\n",
    "    satCounts[satCount].append(clauseAtom)\n",
    "satOrdered = sorted(((len(group),group) for count,group in satCounts.items()), reverse = True)\n",
    "\n",
    "# B.\n",
    "catOrdereds = list()\n",
    "for satCount, clauseAtoms in satOrdered:\n",
    "    catCounts = defaultdict(list)\n",
    "    for ca in clauseAtoms:\n",
    "        cats = ''\n",
    "        for satellite, satData in ckbSatellites[ca].items():\n",
    "            cats += ' '.join(satData['categories'])\n",
    "        catCounts[cats].append(ca)\n",
    "    catOrdered = sorted(((satCount, len(group), group) for group in catCounts.values()), reverse = True)\n",
    "    catOrdereds.extend(catOrdered)\n",
    "\n",
    "# C.\n",
    "prepOrdereds = list()\n",
    "for satCount, catCount, clauseAtoms in catOrdereds:\n",
    "    prepCounts = defaultdict(list)\n",
    "    for ca in clauseAtoms:\n",
    "        preps = ''\n",
    "        for satellite, satData in ckbSatellites[ca].items():\n",
    "            preps += ' '.join(F.lex_utf8.v(po[0]) if type(po[0]) == int else po[0]\n",
    "                              for po in satData['prepObjcs'])\n",
    "        prepCounts[preps].append(ca)\n",
    "    prepOrdered = sorted(((satCount, catCount, len(group), group) \n",
    "                         for group in prepCounts.values()), reverse = True)\n",
    "    prepOrdereds.extend(prepOrdered)\n",
    "    \n",
    "writeOrder = prepOrdereds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3.-4. Gather data and write to csv\n",
    "\n",
    "fieldnames = ['reference','clauseAtom','Verse','Rare Terms','Clause']\n",
    "satelliteFields = ['SATELLITE: ','prepCount','ObjCount','Function','Category','(Preposition) + Objects']\n",
    "satelliteCount = max(satCounts)\n",
    "for satCount in range(0, satelliteCount):\n",
    "    fieldnames.extend(satelliteFields)\n",
    "    \n",
    "rowFormat = ''\n",
    "\n",
    "with open('CKB_valency_groups.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(fieldnames)\n",
    "    \n",
    "    for satCount, catCount, prepCount, clauseAtoms in writeOrder:\n",
    "        for ca in clauseAtoms:\n",
    "            reference = T.sectionFromNode(ca)\n",
    "            verse = T.text(L.d(L.u(ca, otype='verse')[0], otype='word'))\n",
    "            lexWords = tuple(L.u(w,otype='lex')[0] for w in L.d(ca,otype='word'))\n",
    "            rareTerms = '\\n '.join(str((F.lex.v(l),F.gloss.v(l))) for l in lexWords if F.freq_lex.v(l) < 30)\n",
    "            clause = T.text(L.d(ca, otype='word'))\n",
    "            \n",
    "            row = list((reference,\n",
    "                        ca,\n",
    "                        verse,\n",
    "                        rareTerms,\n",
    "                        clause))\n",
    "            \n",
    "            for satellite, satDat in ckbSatellites[ca].items():\n",
    "                \n",
    "                \n",
    "                objects = ' '.join(F.g_word_utf8.v(w) or w for prepObj in satDat['prepObjcs']\n",
    "                          for w in prepObj) if satDat['prepObjcs'] else \\\n",
    "                          ' '.join(F.g_word_utf8.v(w) for w in satDat['objects']) if satDat['objects'] else\\\n",
    "                          'none'\n",
    "                \n",
    "                satColumn = list((satellite,\n",
    "                                  len(satDat['prepObjcs']),\n",
    "                                  len(satDat['objects']),\n",
    "                                  F.function.v(satellite),\n",
    "                                  ' '.join(satDat['categories']),\n",
    "                                  objects\n",
    "                                  ))\n",
    "                row.extend(satColumn)\n",
    "            \n",
    "            writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
