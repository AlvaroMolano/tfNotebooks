{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Wordlists from BDB to ETCBC\n",
    "\n",
    "The goal of this notebook is to map lexemes already categorised into categories useful for valency research from BDB to the ETCBC database. Each lexeme will be converted to the ETCBC transliterated [`lex` feature](https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/lex.html) to facilitate its use with Text-Fabric.\n",
    "\n",
    "This notebook uses the part of speech list generated in [bdb2etcbc_pos_sorting.ipynb](https://github.com/codykingham/textfabric_notebooks/blob/master/valency_wordlists/bdb2etcbc_pos_sorting.ipynb)\n",
    "\n",
    "The source for the BDB resource is openscriptures' [BrownDriverBriggs.xml](https://github.com/openscriptures/HebrewLexicon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.1.3\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/0_overview.html\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "108 features found and 0 ignored\n",
      "\n",
      "  0.00s loading features ...\n",
      "   |     0.04s B otype                from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.18s B g_cons_utf8          from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.18s B g_lex_utf8           from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.20s B g_word_utf8          from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.15s B lex                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.05s T voc_utf8             from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B gloss                from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.00s Feature overview: 103 nodes; 4 edges; 1 configs; 7 computeds\n",
      "  4.50s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "from tf.fabric import Fabric\n",
    "TF = Fabric(modules='Hebrew/etcbc4c')\n",
    "print()\n",
    "api = TF.load('otype lex g_lex_utf8 voc_utf8 g_word_utf8 g_cons_utf8 gloss')\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('pos', 'n.pr.gent'), ('type', 'agent'), ('of kind', 'person')]),\n",
       " OrderedDict([('pos', 'n.pl.m'), ('type', 'object'), ('of kind', 'abstract')]),\n",
       " OrderedDict([('pos', 'n.pr.font'), ('type', 'place'), ('of kind', 'name')]),\n",
       " OrderedDict([('pos', 'n.pr.pl.gent.'),\n",
       "              ('type', 'agent'),\n",
       "              ('of kind', 'person')]),\n",
       " OrderedDict([('pos', 'n.pr.pers.m'),\n",
       "              ('type', 'agent'),\n",
       "              ('of kind', 'person')])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import csv\n",
    "import collections as col\n",
    "\n",
    "tree = etree.parse(\"BrownDriverBriggs.xml\")\n",
    "root = tree.getroot()\n",
    "namespace = {'None':'http://openscriptures.github.com/morphhb/namespace'}\n",
    "\n",
    "with open('BDB_pos_tags.csv','r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    pos_tags = list(dic for dic in reader)\n",
    "    \n",
    "pos_tags[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('(ו)יעשׂו', {'category': 'agent', 'pos': 'n.pr.m', 'subcategory': 'half'}),\n",
       " ('(וְ)יַעֲזִיאֵל',\n",
       "  {'category': 'agent', 'pos': 'n.pr.m', 'subcategory': 'half'}),\n",
       " (']עֵת] קָצִין',\n",
       "  {'category': 'place', 'pos': 'n.pr.loc', 'subcategory': 'kind/name'}),\n",
       " ('אֱדוֹם', {'category': 'agent', 'pos': 'n.pr.m', 'subcategory': 'half'}),\n",
       " ('אֱוִי', {'category': 'agent', 'pos': 'n.pr.m', 'subcategory': 'half'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gather all of the lemmas to test against ETCBC\n",
    "\n",
    "lemmaToPos = {}\n",
    "\n",
    "for tag in pos_tags:\n",
    "    pos = tag['pos']\n",
    "    typ = tag['type']\n",
    "    kind = tag['of kind']\n",
    "    for entry in root.findall('None:part/None:section/None:entry/None:pos', namespace):\n",
    "        cur_pos = entry.text\n",
    "        if cur_pos == pos:\n",
    "            parent = entry.getparent()\n",
    "            text = parent.findall('None:w', namespace)[0]\n",
    "            lemmaToPos[text.text] = {'pos' : pos, 'category': tag['type'], 'subcategory': tag['of kind']}\n",
    "\n",
    "print(len(lemmaToPos))\n",
    "sorted(lemmaToPos.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collect_letters():\n",
    "    '''\n",
    "    returns all consonants/vowels from etcbc\n",
    "    omits diacritical marks\n",
    "    '''\n",
    "    consonants = set()\n",
    "    vowels = set()\n",
    "    sample_words = F.otype.s('word')\n",
    "    for word in sample_words:\n",
    "        for letter in F.g_cons_utf8.v(word):\n",
    "            if letter not in {' ','ׁ','ׂ'}:\n",
    "                consonants.add(letter)\n",
    "    for word in sample_words:\n",
    "        for letter in F.g_lex_utf8.v(word):\n",
    "            if letter not in consonants and letter not in {' '}:\n",
    "                vowels.add(letter)\n",
    "    return {'consonants' : consonants, 'vowels' : vowels}\n",
    "            \n",
    "def strip_diacritic(word, consonants, vowels):\n",
    "    '''\n",
    "    strip diacritical markings and return clean word\n",
    "    '''\n",
    "    new_word = ''\n",
    "    for letter in word:\n",
    "        if letter in consonants or letter in vowels:\n",
    "            new_word += letter\n",
    "            \n",
    "    return new_word\n",
    "\n",
    "def fix_holem(word):\n",
    "    '''\n",
    "    fixes a vocalisation error on etcbc4c words:\n",
    "    ex: 'גֹּויִם into גּוֹיִם'\n",
    "    '''\n",
    "    if 'ֹו' in word:\n",
    "        return word.replace('ֹ', '').replace('ו','וֹ')\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def strip_dagesh(word):\n",
    "    '''\n",
    "    remove first dagesh from word\n",
    "    '''\n",
    "    if len(word) > 1 and word[1] == 'ּ': #dagesh\n",
    "        return word.replace('ּ','',1)\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "def fix_word(word):\n",
    "    '''\n",
    "    apply diacritical stripping and other corrections\n",
    "    return clean word\n",
    "    '''\n",
    "    clean_word = strip_diacritic(word, letters['consonants'], letters['vowels'])\n",
    "    clean_word = fix_holem(clean_word)\n",
    "    clean_word = strip_dagesh(clean_word)\n",
    "    return clean_word\n",
    "    \n",
    "def text_to_lex():\n",
    "    '''\n",
    "    creates mapping from a cleaned etcbc word\n",
    "    to the corresponding etcbc lex features\n",
    "    '''\n",
    "    text_dict = col.defaultdict(set)\n",
    "    for word in F.otype.s('word'):\n",
    "        clean_word = fix_word(F.g_word_utf8.v(word))\n",
    "        lex = F.lex.v(word)\n",
    "        text_dict[F.g_lex_utf8.v(word)].add(lex)\n",
    "        text_dict[clean_word].add(lex)\n",
    "    return text_dict\n",
    "    \n",
    "def match_etcbc(bdb_lex, bib_lex):\n",
    "    '''\n",
    "    matches bdb lexemes with etcbc lexemes\n",
    "    requires the text_to_lex dict\n",
    "    '''\n",
    "    clean_bdb_lex = fix_word(bdb_lex) # the bdb lexs occasionally have diacriticals too!\n",
    "    if clean_bdb_lex in bib_lex:\n",
    "        return bib_lex[clean_bdb_lex]\n",
    "    \n",
    "letters = collect_letters() # required for fix_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# complete mapping from an etcbc clean word to its corresponding ascii lemmas\n",
    "etcbcLex = text_to_lex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make the final mapping from a etcbc lex ascii lex feature to its category: agency, place, or object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2079"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BdbMappedEtcbc = {}\n",
    "mapped = set()\n",
    "\n",
    "for lex, posData in lemmaToPos.items():\n",
    "    etcbcLexFeature = match_etcbc(lex, etcbcLex)\n",
    "    if not etcbcLexFeature: continue\n",
    "    for feat in etcbcLexFeature: \n",
    "        BdbMappedEtcbc[feat] = posData\n",
    "    mapped.add(lex)\n",
    "len(mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not yet mapped to ETCBC:  247\n"
     ]
    }
   ],
   "source": [
    "print('Not yet mapped to ETCBC: ', len(set(lemmaToPos.keys()) - mapped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems in the mapping so far:\n",
    "* Many issues seem to be caused by proper nouns\n",
    "* differences in vocalization\n",
    "\n",
    "Fixed problems:\n",
    "* √ removed diacritical markers from both etcbc and bdb lemmas (`clean_word()`)\n",
    "    * brought the un-mapped down from ~700 to ~350\n",
    "* √ removed first position dageshes to solve pointing discrepancies\n",
    "    * unmapped down from ~350 to 247\n",
    "    \n",
    "Any given lemma might contain a mapping to more than one etcbc lex feature. This is unfavorable in cases where the word form may be exact, but the sense is different from that intended by the part of speech tag that we've chosen to keep. That the vocalised text is more specific has been a motivating factor for keeping the vowels. In order to test the spread of lex objects in the matches, let's average the length of matches per lemma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lexemes:  2079\n",
      "Average len of ascii lemma mapping:  1.28\n"
     ]
    }
   ],
   "source": [
    "total_lemma = 0\n",
    "len_lex = 0\n",
    "\n",
    "for lemma in mapped:\n",
    "    total_lemma += 1\n",
    "    len_lex += len(match_etcbc(lemma, bib_lex))\n",
    "    \n",
    "print('total lexemes: ', total_lemma)\n",
    "print('Average len of ascii lemma mapping: ', round(len_lex/total_lemma, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good that the number is close to 1.0 since that means there are less double(+)-matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# export the completed list\n",
    "import json\n",
    "\n",
    "with open('bdbCategories.json', 'w') as file:\n",
    "    json.dump(BdbMappedEtcbc, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
