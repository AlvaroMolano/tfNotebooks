{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='text-align: center; font-size:300%'>`שכב` Valency</h1>\n",
    "<p style ='text-align: center;'><strong>What factors determine how the verb שכב ('to lie') is translated?</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook last modified on 2017-01-15 22:42:43.568100\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print('Notebook last modified on {}'.format(datetime.now().__str__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "The goal is to inventory and categorise the various satellites surrounding the verb שכב (\"to lie\") in biblical Hebrew in order to determine which elements give rise to which semantic meanings of שכב. Valency tracks the interaction between semantics and syntax.\n",
    "\n",
    "In Dyk et al. they suggest that few \"watertight\" methods exist to separate obligatory, complement functions from non-obligatory, adjunctive functions. (see [Dyk, Glanz, Oosting, \"Analysing Valence Patterns,\"](https://shebanq.ancient-data.org/shebanq/static/docs/methods/2014_Dyk_jnsl.pdf) 4-5). They apply a \"distributional method\" as follows:\n",
    "\n",
    "* \"Collect all occurrences of a verb with the complete patterns of elements occurring in the data.\"\n",
    "* \"Sort these by pattern.\"\n",
    "* \"Analyse the differences between the various patterns, observing what relation the separate sentence constituents have to the verb.\" *(Dyk et al., 6)*\n",
    "\n",
    "For this notebook, adjuncts and complements are treated more or less the same since some of the categories are mis-labeled in etcbc4c. \n",
    "\n",
    "The [Roorda/Dyk valency corrections notebook](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/corr_enrich.html) informed and inspired many of the queries created in this notebook.\n",
    "\n",
    "The [CKB_valency_rough.ipynb](https://github.com/codykingham/textfabric_notebooks/tree/master/שכב%20valency/preliminary/CKB_valency_rough.ipynb) is the predecessor to this one. There, satellite groups are organised based on rough, basic parameters like the presence or absence of a preposition. The rough spreadsheets exported by that notebook form the background for the rules used in this notebook. \n",
    "\n",
    "## Procedures\n",
    "\n",
    "### 1. Gather clauses in which  שכב  serves the predicate role.\n",
    "### 2. Process the clause's satellites based on a checklist of rules; these include checking for various idioms, prepositions, and noun types.\n",
    "### 3. Export the clauses and their satellites to a segmented, organised spreadsheet for further analysis.\n",
    "### 4. See the final results [here](https://github.com/codykingham/textfabric_notebooks/tree/master/שכב%20valency/CKB_valency_groups.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.3.0\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "109 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.04s B otype                from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B book                 from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B chapter              from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B verse                from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.20s B g_cons_utf8          from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.34s B g_word_utf8          from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.26s B lex_utf8             from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.07s B function             from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.14s B pdp                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.14s B sp                   from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.13s B vs                   from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.14s B prs                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.13s B st                   from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.30s B rela                 from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.15s B lex                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.09s B g_prs_utf8           from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.30s B nu                   from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.00s B nametype             from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.18s B ls                   from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.01s B gloss                from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.25s B uvf                  from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.00s B freq_lex             from /Users/Cody/github/text-fabric-data/Hebrew/etcbc4c\n",
      "   |     0.00s Feature overview: 103 nodes; 5 edges; 1 configs; 7 computeds\n",
      "  7.89s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "# load and import necessary modules\n",
    "\n",
    "import json, csv\n",
    "from collections import *\n",
    "from pprint import pprint\n",
    "from tf.fabric import Fabric\n",
    "\n",
    "TF = Fabric(modules='Hebrew/etcbc4c')\n",
    "api = TF.load(\"\"\"otype\n",
    "                 book chapter verse\n",
    "                 function pdp sp vs prs st rela\n",
    "                 lex g_cons_utf8 g_word_utf8 g_prs_utf8 nu\n",
    "                 lex_utf8\n",
    "                 nametype ls gloss uvf\n",
    "                 freq_lex\n",
    "                \"\"\")\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gather clauses in which  שכב  serves the predicate role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 'CKB['\n",
    "stem = 'qal'\n",
    "predicateFunctions = {'Pred', 'PreS', 'PreO', 'PreC', 'PtcO', 'PrcS'} \n",
    "\n",
    "ckbClauses = set(L.u(w, otype='clause_atom')[0] for w in F.otype.s('word') \n",
    "                     if F.lex.v(w) == target \n",
    "                     and F.vs.v(w) == stem\n",
    "                     and F.function.v(L.u(w, otype='phrase')[0]) in predicateFunctions\n",
    "                )\n",
    "len(ckbClauses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process the clause's satellites based on a checklist of rules; these include checking for various idioms, prepositions, and noun types.\n",
    "\n",
    "We need a number of resources to accomplish this.\n",
    "\n",
    "**First**, we use a sense generator that will return 1 of 3 categories for a given lemma. The categories are:\n",
    "* person\n",
    "* place\n",
    "* object (if it is 'abstract')\n",
    "\n",
    "The functions use 4 sources to make the decisions: \n",
    "1. existing features in the ETCBC\n",
    "2. lists of categorised lexemes from the [Roorda/Dyk notebook](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/corr_enrich.html)\n",
    "3. sources 3 and 4 are special projects created for these kinds of queries:\n",
    "    * see [valency_wordlists](https://github.com/codykingham/textfabric_notebooks/blob/master/valency_wordlists)\n",
    "    * (3) generated category lists from [openscriptures' BDB lexicon](https://github.com/openscriptures/HebrewLexicon) using part-of-speech tags in BDB \n",
    "    * (4) generated category lists from Wordnet, using hypernym relations\n",
    "\n",
    "Each of the sources will count as 1 or more parameters, that, if fulfilled, will go towards a score for the given lexeme object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sense Generators: \n",
    "\n",
    "*TO-DO: combine the 3 generators into 1 simplified function (15.01.17)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../valency_wordlists/bdbCategories.json') as bdbFile:\n",
    "    bdbCategories = json.load(bdbFile)\n",
    "with open('../valency_wordlists/wordnetCategories.json') as wnFile:\n",
    "    wnCategories = json.load(wnFile)\n",
    "with open('../valency_wordlists/RoordaDykCategories.json') as rdFile:\n",
    "    rdCategories = json.load(rdFile)    \n",
    "\n",
    "def testLocation(word_node, info=False):\n",
    "    lexeme = F.lex.v(word_node)\n",
    "    # SOURCE 1, ETCBC4c\n",
    "    lex_obj = L.u(word_node, otype = 'lex')[0]\n",
    "    univalent_final = 1 if F.uvf.v(word_node) == 'H' else 0\n",
    "    name_type = 1 if F.nametype.v(lex_obj) == 'topo' else 0\n",
    "    s1_score = sum((univalent_final, name_type))\n",
    "    # SOURCE 2, ROORDA/DYK\n",
    "    s2_score = 1 if lexeme in rdCategories and rdCategories[lexeme]['category'] == 'place' else 0\n",
    "    # SOURCE 3, BDB categories list\n",
    "    s3_score = 1 if lexeme in bdbCategories and bdbCategories[lexeme]['category'] == 'place' else 0\n",
    "    # source 4, wordnet categories list\n",
    "    s4_score = 1 if lexeme in wnCategories and wnCategories[lexeme]['cat'] == 'place' else 0\n",
    "    if not info: return sum((s1_score, s2_score, s3_score, s4_score))\n",
    "    else: return { 's1':s1_score, 's2':s2_score,'s3':s3_score, 's4':s4_score}\n",
    "    \n",
    "def testAgency(wordNode, info=False):\n",
    "    # SOURCE 1, ETCBC4c\n",
    "    lexN = L.u(wordNode, otype = 'lex')[0]\n",
    "    lex = F.lex.v(lexN)\n",
    "    nametype = 1 if F.nametype.v(lexN) == 'pers' else 0\n",
    "    gentilic = 1 if F.ls.v(wordNode) == 'gntl' else 0\n",
    "    pronoun = 1 if F.sp.v(wordNode) == 'prps' or F.pdp.v(wordNode) == 'prps' else 0\n",
    "    s1Score = sum((nametype,gentilic,pronoun))\n",
    "    # SOURCE 2, Roorda/Dyk\n",
    "    s2Score = 1 if lex in rdCategories and rdCategories[lex]['category'] == 'agent' else 0\n",
    "    # Source 3 - BDB categories\n",
    "    s3Score = 1 if lex in bdbCategories and bdbCategories[lex]['category'] == 'agent' else 0\n",
    "    # Source 4 - WN categories\n",
    "    s4Score = 1 if lex in wnCategories and wnCategories[lex]['cat'] == 'agent' else 0\n",
    "    if not info: return sum((s1Score, s2Score, s3Score, s4Score))\n",
    "    else: return { 's1':s1Score, 's2':s2Score,'s3':s3Score, 's4':s4Score}\n",
    "    \n",
    "def abstractObject(wordNode, info=False):\n",
    "    lex = F.lex.v(wordNode)\n",
    "    # Source 1, BDB categories\n",
    "    s1score = 1 if lex in bdbCategories and bdbCategories[lex]['subcategory'] == 'abstract' else 0\n",
    "    s2score = 1 if lex in wnCategories and wnCategories[lex]['subcategory'] == 'abstract' else 0\n",
    "    if not info: return(sum((s1score, s2score)))\n",
    "    else: return {'BDB': s1score, 'WordNet':s2score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules for processing satellites (phrases):\n",
    "\n",
    "**Second, **the groups of satellite types are created based on the following set of rules. Each rule results in a given sense which, based on the syntax, represents the most likely interpretation of the verb שכב in a given passage.\n",
    "\n",
    "* preposition\n",
    "    * עם / את\n",
    "        * object == pl אב ('fathers') ?\n",
    "            * **death, figurative idiom**\n",
    "        * object == suffix ?\n",
    "            * **sexual intercourse**\n",
    "        * object == agentive noun ?\n",
    "            * locative satellite in between ? \n",
    "                * **literal action, with/among entity** \n",
    "            * else: \n",
    "                * **sexual intercourse**\n",
    "        * else\n",
    "            * **literal action, with/among entity**\n",
    "\n",
    "    * על / ל / ב / אצל / בין, et al.\n",
    "        * object == abstract object ? \n",
    "            * **literal action, manner of lying**\n",
    "        * object == locative or unknown ?\n",
    "            * ** literal action, spatial direction of act**\n",
    "\n",
    "* ø preposition\n",
    "    * noun/adj/adv == locative ?\n",
    "        * 'sex' category in clause atom? \n",
    "            * **sexual intercourse + adjunctive object**\n",
    "        * else:\n",
    "            * **literal action, spatial direction of act**\n",
    "    * ø noun/adj/adv\n",
    "        * +verbal suffix ?\n",
    "            * **sexual intercourse** \n",
    "        * else\n",
    "            * **literal action, connotates sleep**\n",
    "        \n",
    "        \n",
    "These rules are applied on a per-phrase basis to create phrase categories. Categories can be compounded together to create compound categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isObjectNoun(w):\n",
    "    '''\n",
    "    returns whether a given word is an \"object noun\"\n",
    "    the term is used loosely to refer to words that:\n",
    "    1. function as objects of prepositions, or\n",
    "    2. direct the force of the verb in some way\n",
    "    See for example the particle שם ('there')\n",
    "    '''\n",
    "    if all([F.pdp.v(w) in {'subs','prps','prin','adjv','advb','nmpr'},\n",
    "            not {'rec','atr'} & set(F.rela.v(subPhrase) for subPhrase in L.u(w, otype='subphrase')),\n",
    "            F.function.v((L.u(w,otype='phrase'))) != 'Subj',\n",
    "            F.rela.v(L.u(w, otype = 'phrase_atom')[0]) not in {'Appo'},\n",
    "           ]):\n",
    "        return True\n",
    "\n",
    "def analyseSatellites(clauseAtom):\n",
    "    '''\n",
    "    applies the rules for the semantic interpretation of the verb CKB\n",
    "    in a certain clause atom, based on the presence of certain satellite types;\n",
    "    in some cases, the presence of additional satellites condition the\n",
    "    meaning of another satellite\n",
    "    \n",
    "    The rules follow closely those outlined in the markdown cell above, although\n",
    "    the resulting tags are more nuanced.\n",
    "    \n",
    "    The function returns nested dictionaries that contain satellites as keys, each with\n",
    "    their own dictionaries that contain additional keys and data on the satellite. These \n",
    "    are: preposition + their object, function of the phrase (satellite), categories determined\n",
    "    by the function, and object \"nouns\" within the clause\n",
    "    '''\n",
    "    satelliteFunctions = {'Adju','PreO','Objc','Cmpl','Loca'}\n",
    "    satellites = (ph for ph in L.d(clauseAtom, otype='phrase') if F.function.v(ph) in satelliteFunctions)\n",
    "    satData = defaultdict(dict)\n",
    "    \n",
    "    for sat in satellites:\n",
    "        words = L.d(sat, otype='word')\n",
    "        prepositions = tuple(w for w in words if F.pdp.v(w) == 'prep')\n",
    "        objectNouns = tuple(w for w in words if isObjectNoun(w))\n",
    "        categories = []\n",
    "        prepAndObjs = []\n",
    "        \n",
    "        # apply the rules following the logic in the above notes\n",
    "        if prepositions:\n",
    "                # get suffixed objects from suffixed prepositions or keep preposition objects\n",
    "            objectNouns = tuple(F.g_prs_utf8.v(prep) for prep in prepositions) \\\n",
    "                          if not objectNouns else objectNouns\n",
    "            prepAndObj = tuple(zip(prepositions, objectNouns)) # map prepositions to their objs\n",
    "            \n",
    "            # Calculate the categories for prepositions:    \n",
    "            for prep, pObj in prepAndObj:\n",
    "                if F.lex.v(prep) in {'<M','>T','>T=='}:\n",
    "                    if F.lex.v(pObj) == '>B/' and F.nu.v(pObj) == 'pl': \n",
    "                        category = 'death'\n",
    "                    elif type(pObj) == str or testAgency(pObj): # suffix obj or agentive obj.\n",
    "                        \n",
    "                        # determine whether an intervening preposional phrase occurs\n",
    "                        # such a phrase can block the sexual meaning as seen in texts like:\n",
    "                        # Job 20.11 and 2 Sam 11.3\n",
    "                        verb = next(w for w in L.d(clauseAtom, otype='word') if F.pdp.v(w) == 'verb')\n",
    "                        first, second = sorted((verb,prep)) # verb can come before or after\n",
    "                        # now calculate all the intervening words and include if one is a preposition\n",
    "                        # but it cannot be a 'with' preposition\n",
    "                        interveningToVerb = set(F.pdp.v(n) for n in range(first+1, second)\n",
    "                                               if F.lex.v(n) not in {'<M','>T','>T=='})\n",
    "                        if any(['prep' in interveningToVerb,\n",
    "                                'literal.prepSpatial' in {cat for sat in satData[clauseAtom]\n",
    "                                                          for cat in satData[clauseAtom][sat]['categories']}\n",
    "                               ]):\n",
    "                            category = 'literal.withEntity'\n",
    "                        else:\n",
    "                            category = 'sex'\n",
    "                    else:\n",
    "                        category = 'literal.withEntity'\n",
    "                else: #'<L','B','L','>YL/','BJN/', et. al\n",
    "                    if abstractObject(pObj) or F.lex.v(prep) == 'K':\n",
    "                        category = 'literal.manner'\n",
    "                    else:\n",
    "                        category = 'literal.prepSpatial'\n",
    "                categories.append(category)\n",
    "                prepAndObjs.extend(prepAndObj)\n",
    "        elif objectNouns:\n",
    "            for obj in objectNouns:\n",
    "                if testLocation(obj):\n",
    "                    category = 'literal.spatialObj'\n",
    "                else:\n",
    "                    if any(['sex' in categories,\n",
    "                            'sex' in {cat for sat in satData[clauseAtom]\n",
    "                                      for cat in satData[clauseAtom][sat]['categories']}]):\n",
    "                        category = 'sex.object'\n",
    "                    else:\n",
    "                        category = 'literal.unknown'\n",
    "                categories.append(category)\n",
    "        else:\n",
    "            verb = tuple(w for w in words if F.function.v(L.u(w, otype='phrase')[0]) == 'PreO')\n",
    "            if verb and F.prs.v(verb[0]):\n",
    "                category = 'sex'\n",
    "            else:\n",
    "                category = 'literal.general'\n",
    "            categories.append(category)\n",
    "        \n",
    "        satData[clauseAtom][sat] = {'prepObjcs': prepAndObjs,\n",
    "                                    'function' : F.function.v(sat),\n",
    "                                    'categories' : categories,\n",
    "                                    'objects' : objectNouns\n",
    "                                   }\n",
    "    if not satData:\n",
    "        satData[clauseAtom] = {}\n",
    "    return satData        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the CKB clauses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ckbSatellites = {}\n",
    "for clauseAtom in ckbClauses:\n",
    "    ckbSatellites.update(analyseSatellites(clauseAtom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(536065,\n",
      "  {668165: {'categories': ['death'],\n",
      "            'function': 'Cmpl',\n",
      "            'objects': (111042,),\n",
      "            'prepObjcs': [(111041, 111042)]}}),\n",
      " (551428,\n",
      "  {713724: {'categories': ['death'],\n",
      "            'function': 'Cmpl',\n",
      "            'objects': (188497,),\n",
      "            'prepObjcs': [(188496, 188497)]}})]\n"
     ]
    }
   ],
   "source": [
    "# see an example of the analyser output here:\n",
    "pprint(list(ckbSatellites.items())[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Export the clauses and their satellites to a segmented, organised spreadsheet for further analysis.\n",
    "1. organise the groups based on 3 tiers of subcategorisation: \n",
    "   * A. the most prevalent satellite counts within all clauses\n",
    "   * B. the most prevalent satellite categories and category combinations within A\n",
    "   * C. the most prevalent prepositional phrases within B\n",
    "2. gather reference information, plain text, labels, etc.; write to the csv file\n",
    "\n",
    "### organise the groups based on 3 tiers of subcategorisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A.\n",
    "satCounts = defaultdict(list)\n",
    "for clauseAtom, satellites in ckbSatellites.items():\n",
    "    satCount = len(satellites)\n",
    "    satCounts[satCount].append(clauseAtom)\n",
    "satOrdered = sorted(((len(group),group) for count,group in satCounts.items()), reverse = True)\n",
    "\n",
    "# B.\n",
    "catOrdereds = list()\n",
    "for satCount, clauseAtoms in satOrdered:\n",
    "    catCounts = defaultdict(list)\n",
    "    for ca in clauseAtoms:\n",
    "        cats = ''\n",
    "        for satellite, satData in ckbSatellites[ca].items():\n",
    "            cats += ' '.join(satData['categories'])\n",
    "        catCounts[cats].append(ca)\n",
    "    catOrdered = sorted(((satCount, len(group), group) for group in catCounts.values()), reverse = True)\n",
    "    catOrdereds.extend(catOrdered)\n",
    "\n",
    "# C.\n",
    "prepOrdereds = list()\n",
    "for satCount, catCount, clauseAtoms in catOrdereds:\n",
    "    prepCounts = defaultdict(list)\n",
    "    for ca in clauseAtoms:\n",
    "        preps = ''\n",
    "        for satellite, satData in ckbSatellites[ca].items():\n",
    "            preps += ' '.join(F.lex_utf8.v(po[0]) if type(po[0]) == int else po[0]\n",
    "                              for po in satData['prepObjcs'])\n",
    "        prepCounts[preps].append(ca)\n",
    "    prepOrdered = sorted(((satCount, catCount, len(group), group) \n",
    "                         for group in prepCounts.values()), reverse = True)\n",
    "    prepOrdereds.extend(prepOrdered)\n",
    "    \n",
    "writeOrder = prepOrdereds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gather reference information, plain text, labels, etc.; write to the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fieldnames = ['reference','clauseAtom','Verse','Rare Terms','Clause','SatCount']\n",
    "satelliteFields = ['SATELLITE: ','prepCount','ObjCount','Function','Category','(Preposition) + Objects']\n",
    "satelliteCount = max(satCounts)\n",
    "for satCount in range(0, satelliteCount):\n",
    "    fieldnames.extend(satelliteFields)\n",
    "\n",
    "with open('CKB_valency_groups.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(fieldnames)\n",
    "    \n",
    "    for satCount, catCount, prepCount, clauseAtoms in writeOrder:\n",
    "        for ca in clauseAtoms:\n",
    "            reference = T.sectionFromNode(ca)\n",
    "            verse = T.text(L.d(L.u(ca, otype='verse')[0], otype='word'))\n",
    "            lexWords = tuple(L.u(w,otype='lex')[0] for w in L.d(ca,otype='word'))\n",
    "            rareTerms = '\\n '.join(str((F.lex.v(l),F.gloss.v(l))) for l in lexWords if F.freq_lex.v(l) < 30)\n",
    "            clause = T.text(L.d(ca, otype='word'))\n",
    "            \n",
    "            row = list((reference,\n",
    "                        ca,\n",
    "                        verse,\n",
    "                        rareTerms,\n",
    "                        clause,\n",
    "                        len(ckbSatellites[ca])\n",
    "                      ))\n",
    "            \n",
    "            for satellite, satDat in ckbSatellites[ca].items():\n",
    "                \n",
    "                \n",
    "                objects = ' '.join(F.g_word_utf8.v(w) or w \n",
    "                                   for prepObj in satDat['prepObjcs']\n",
    "                                   for w in prepObj) \\\n",
    "                                   if satDat['prepObjcs'] \\\n",
    "                                   else ' '.join(F.g_word_utf8.v(w) for w in satDat['objects']) \\\n",
    "                                   if satDat['objects']\\\n",
    "                                   else 'none'\n",
    "\n",
    "                satColumn = list((satellite,\n",
    "                                  len(satDat['prepObjcs']),\n",
    "                                  len(satDat['objects']),\n",
    "                                  F.function.v(satellite),\n",
    "                                  ' '.join(satDat['categories']),\n",
    "                                  objects\n",
    "                                  ))\n",
    "                row.extend(satColumn)\n",
    "                \n",
    "            # fill in empty columns\n",
    "            for column in range(len(fieldnames) - len(row)): \n",
    "                row.append('')\n",
    "            \n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Browse the final results at:\n",
    "\n",
    "[CKB_valency_groups.csv](https://github.com/codykingham/textfabric_notebooks/tree/master/שכב%20valency/CKB_valency_groups.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
